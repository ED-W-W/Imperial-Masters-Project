{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab03635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import chex\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any, Dict\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "\n",
    "from jaxmarl import make\n",
    "from jaxmarl.wrappers.baselines import (\n",
    "    SMAXLogWrapper,\n",
    "    MPELogWrapper,\n",
    "    LogWrapper,\n",
    "    CTRolloutManager,\n",
    ")\n",
    "from jaxmarl.environments.smax import map_name_to_scenario, HeuristicEnemySMAX\n",
    "from jaxmarl.viz.visualizer import Visualizer, SMAXVisualizer\n",
    "\n",
    "import functools\n",
    "from functools import partial\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qdax.core.map_elites import MAPElites\n",
    "from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids, MapElitesRepertoire\n",
    "#import qdax.tasks.brax.v1 as environments\n",
    "#from qdax.tasks.brax.v1.env_creators import scoring_function_brax_envs as scoring_function\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.emitters.standard_emitters import MixingEmitter\n",
    "from qdax.utils.plotting import plot_map_elites_results\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "\n",
    "## Brax version conflict with JaxMARL(0.10.3) and QDax(0.10.4 / 0.12.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13706c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Tuple\n",
    "from qdax.custom_types import (\n",
    "    Descriptor,\n",
    "    EnvState,\n",
    "    ExtraScores,\n",
    "    Fitness,\n",
    "    Genotype,\n",
    "    Observation,\n",
    "    Params,\n",
    "    RNGKey,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # valid for iql, vdn, qmix\n",
    "    \"TOTAL_TIMESTEPS\": 1e7,\n",
    "    \"NUM_ENVS\": 1, #16,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"BUFFER_SIZE\": 10000, #5000,\n",
    "    \"BUFFER_BATCH_SIZE\": 32,\n",
    "    \"HIDDEN_SIZE\": 128, #512,\n",
    "    \"MIXER_EMBEDDING_DIM\": 64,\n",
    "    \"MIXER_HYPERNET_HIDDEN_DIM\": 256,\n",
    "    \"MIXER_INIT_SCALE\": 0.001,\n",
    "    \"EPS_START\": 1.0,\n",
    "    \"EPS_FINISH\": 0.05,\n",
    "    \"EPS_DECAY\": 0.1, # percentage of updates\n",
    "    \"MAX_GRAD_NORM\": 10,\n",
    "    \"TARGET_UPDATE_INTERVAL\": 5, #10,\n",
    "    \"TAU\": 1.,\n",
    "    \"NUM_EPOCHS\": 8,\n",
    "    \"LR\": 0.00005,\n",
    "    \"LEARNING_STARTS\": 10000, # timesteps\n",
    "    \"LR_LINEAR_DECAY\": False,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"REW_SCALE\": 10., # scale the reward to the original scale of SMAC\n",
    "\n",
    "    # ENV\n",
    "    \"ENV_NAME\": \"HeuristicEnemySMAX\",\n",
    "    #\"MAP_NAME\": \"3s_vs_5z\",\n",
    "    \"MAP_NAME\": \"2s3z\",\n",
    "    \"ENV_KWARGS\": {\n",
    "        \"see_enemy_actions\": True,\n",
    "        \"walls_cause_death\": True,\n",
    "        \"attack_mode\": \"closest\",\n",
    "    },\n",
    "\n",
    "    \"NUM_SEEDS\": 1, # number of vmapped seeds\n",
    "    \"SEED\": 88,\n",
    "\n",
    "    \"HYP_TUNE\": False, # perform hyp tune\n",
    "\n",
    "    # evaluate\n",
    "    \"TEST_DURING_TRAINING\": False, #True,\n",
    "    \"TEST_INTERVAL\": 0.05, # as a fraction of updates, i.e. log every 5% of training process\n",
    "    \"TEST_NUM_STEPS\": 128,\n",
    "    \"TEST_NUM_ENVS\": 512, # number of episodes to average over, can affect performance\n",
    "}\n",
    "batch_size = 64# 128 # Num of offsprings \n",
    "#env_name = 'walker2d_uni'\n",
    "episode_length = config[\"NUM_STEPS\"] #128  # NUM_STEPS\n",
    "num_iterations = int(config[\"TOTAL_TIMESTEPS\"] / (config[\"NUM_ENVS\"] * batch_size * config[\"NUM_STEPS\"]))\n",
    "seed = 88 \n",
    "policy_hidden_layer_sizes = (128, 128, 128) #(64, 64)\n",
    "iso_sigma = 0.05 #0.005 \n",
    "line_sigma = 0.5 #0.05 \n",
    "num_init_cvt_samples = 20000 #50000 \n",
    "num_centroids = 2000 #1024 \n",
    "min_descriptor = 0. \n",
    "max_descriptor = 1.0 \n",
    "number_of_descriptors=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4647e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScannedRNN(nn.Module):\n",
    "\n",
    "    @partial(\n",
    "        nn.scan,\n",
    "        variable_broadcast=\"params\",\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "        split_rngs={\"params\": False},\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        \"\"\"Applies the module.\"\"\"\n",
    "        rnn_state = carry\n",
    "        ins, resets = x\n",
    "        hidden_size = ins.shape[-1]\n",
    "        rnn_state = jnp.where(\n",
    "            resets[:, np.newaxis],\n",
    "            self.initialize_carry(hidden_size, *ins.shape[:-1]),\n",
    "            rnn_state,\n",
    "        )\n",
    "        new_rnn_state, y = nn.GRUCell(hidden_size)(rnn_state, ins)\n",
    "        return new_rnn_state, y\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(hidden_size, *batch_size):\n",
    "        # Use a dummy key since the default state init fn is just zeros.\n",
    "        return nn.GRUCell(hidden_size, parent=None).initialize_carry(\n",
    "            jax.random.PRNGKey(0), (*batch_size, hidden_size)\n",
    "        )\n",
    "\n",
    "\n",
    "class RNNQNetwork(nn.Module):\n",
    "    # homogenous agent for parameters sharing, assumes all agents have same obs and action dim\n",
    "    action_dim: int\n",
    "    hidden_dim: int\n",
    "    init_scale: float = 1.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, obs, dones):\n",
    "        embedding = nn.Dense(\n",
    "            self.hidden_dim,\n",
    "            kernel_init=orthogonal(self.init_scale),\n",
    "            bias_init=constant(0.0),\n",
    "        )(obs)\n",
    "        embedding = nn.relu(embedding)\n",
    "\n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "\n",
    "        q_vals = nn.Dense(\n",
    "            self.action_dim,\n",
    "            kernel_init=orthogonal(self.init_scale),\n",
    "            bias_init=constant(0.0),\n",
    "        )(embedding)\n",
    "\n",
    "        return hidden, q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0cb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init environment\n",
    "#env = environments.create(env_name, episode_length=episode_length)\n",
    "scenario = map_name_to_scenario(config[\"MAP_NAME\"])\n",
    "env = HeuristicEnemySMAX(scenario=scenario, **config[\"ENV_KWARGS\"])\n",
    "env = SMAXLogWrapper(env)\n",
    "wrapped_env = CTRolloutManager(env, batch_size=config[\"NUM_ENVS\"])\n",
    "\n",
    "rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "reset_fn = jax.jit(wrapped_env.batch_reset)\n",
    "#reset_fn = jax.jit(env.reset)\n",
    "\n",
    "config[\"NUM_ACTORS\"] = env.num_agents * config[\"NUM_ENVS\"]\n",
    "config[\"NUM_UPDATES\"] = (\n",
    "    config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Init a random key\n",
    "key = jax.random.key(seed)\n",
    "\n",
    "# Init policy network\n",
    "policy_layer_sizes = policy_hidden_layer_sizes + (env.action_space(env.agents[0]).n,) # Not actually used\n",
    "#policy_network = ActorRNN(env.action_space(env.agents[0]).n, config=config)\n",
    "policy_network = RNNQNetwork(\n",
    "            action_dim=wrapped_env.max_action_space,\n",
    "            hidden_dim=config[\"HIDDEN_SIZE\"],\n",
    "        )\n",
    "\n",
    "# Init population of controllers\n",
    "key, subkey = jax.random.split(key)\n",
    "#keys = jax.random.split(subkey, num=batch_size)\n",
    "# fake_batch = jnp.zeros(shape=(batch_size, env.observation_size)) # Not needed as on-policy is used for no batches?\n",
    "# init_variables = jax.vmap(policy_network.init)(keys, fake_batch)\n",
    "init_x = (\n",
    "    jnp.zeros(\n",
    "        (1, 1, wrapped_env.obs_size)\n",
    "    ),  # (time_step, batch_size, obs_size)\n",
    "    jnp.zeros((1, 1)),  # (time_step, batch size)\n",
    ")\n",
    "init_hstate = ScannedRNN.initialize_carry(\n",
    "    config[\"HIDDEN_SIZE\"], 1\n",
    ")  # (batch_size, hidden_dim)\n",
    "\n",
    "keys = jax.random.split(subkey, batch_size)\n",
    "# Assume init_hstate: [H], init_x: [X]\n",
    "fake_hstate = jnp.stack([init_hstate] * batch_size)  # [N, H]\n",
    "#fake_x = jnp.stack([init_x] * config[\"NUM_ENVS\"])            # [N, X]\n",
    "fake_x = tuple(jnp.stack([x] * batch_size) for x in init_x)\n",
    "\n",
    "#network_params = policy_network.init(subkey, init_hstate, init_x)\n",
    "network_params = jax.vmap(\n",
    "    policy_network.init\n",
    ")(keys, fake_hstate, *fake_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363f8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Transition(NamedTuple):\n",
    "#    global_done: jnp.ndarray\n",
    "#    done: jnp.ndarray\n",
    "#    action: jnp.ndarray\n",
    "#    reward: jnp.ndarray\n",
    "#    obs: jnp.ndarray\n",
    "#    env_state: jnp.ndarray\n",
    "#    info: jnp.ndarray\n",
    "#    avail_actions: jnp.ndarray\n",
    "\n",
    "class Transition(NamedTuple):\n",
    "    global_done: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    env_state: jnp.ndarray\n",
    "\n",
    "@chex.dataclass(frozen=True)\n",
    "class Timestep:\n",
    "    obs: dict\n",
    "    actions: dict\n",
    "    rewards: dict\n",
    "    dones: dict\n",
    "    avail_actions: dict\n",
    "\n",
    "\n",
    "class CustomTrainState(TrainState):\n",
    "    target_network_params: Any\n",
    "    timesteps: int = 0\n",
    "    n_updates: int = 0\n",
    "    grad_steps: int = 0\n",
    "\n",
    "def batchify(x: dict):\n",
    "    return jnp.stack([x[agent] for agent in env.agents], axis=0)\n",
    "\n",
    "def unbatchify(x: jnp.ndarray):\n",
    "    return {agent: x[i] for i, agent in enumerate(env.agents)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "786f024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to play a step with the policy in the environment\n",
    "def play_step_fn(\n",
    "    runner_state\n",
    "):\n",
    "    \"\"\"\n",
    "    Play an environment step and return the updated state and the transition.\n",
    "    \"\"\"\n",
    "    #hs, last_obs, last_dones, env_state, rng = carry\n",
    "    policy_params, env_state, last_obs, last_dones, hstate, rng = runner_state\n",
    "\n",
    "    rng, rng_a, rng_s = jax.random.split(rng, 3)\n",
    "\n",
    "    # (num_agents, 1 (dummy time), num_envs, obs_size)\n",
    "    _obs = batchify(last_obs)[:, np.newaxis]\n",
    "    _dones = batchify(last_dones)[:, np.newaxis]\n",
    "    #_obs = batchify_multi(last_obs)[:, :, np.newaxis]    # [batch, num_agents, 1, num_envs, obs_dim]\n",
    "    #_dones = batchify_multi(last_dones)[:, :, np.newaxis] # [batch, num_agents, 1, num_envs]\n",
    "\n",
    "    print(\"hstate shape:\", hstate.shape)  # should be [BATCH_SIZE, NUM_ENVS, NUM_AGENTS, HIDDEN_SIZE]\n",
    "    print(\"_obs shape:\", _obs.shape)      # should be [BATCH_SIZE, NUM_ENVS, OBS_DIM]\n",
    "    print(\"_dones shape:\", _dones.shape)  # should be [BATCH_SIZE, NUM_ENVS, OBS_DIM]\n",
    "\n",
    "    new_hstate, q_vals = jax.vmap(\n",
    "        policy_network.apply, in_axes=(None, 0, 0, 0)\n",
    "    )(  # vmap across the agent dim\n",
    "        policy_params,\n",
    "        hstate,\n",
    "        _obs,\n",
    "        _dones,\n",
    "    )\n",
    "    print(\"new_hstate shape:\", new_hstate.shape)  # should be [BATCH_SIZE, NUM_ENVS, NUM_AGENTS, HIDDEN_SIZE]\n",
    "\n",
    "    q_vals = q_vals.squeeze(\n",
    "        axis=1\n",
    "    )  # (num_agents, num_envs, num_actions) remove the time dim\n",
    "\n",
    "    # explore\n",
    "    avail_actions = wrapped_env.get_valid_actions(env_state.env_state)\n",
    "    avail_actions_batchified = batchify(avail_actions)\n",
    "\n",
    "    #eps = eps_scheduler(train_state.n_updates)\n",
    "    #_rngs = jax.random.split(rng_a, env.num_agents)\n",
    "    #actions = jax.vmap(eps_greedy_exploration, in_axes=(0, 0, None, 0))(\n",
    "    #    _rngs, q_vals, eps, batchify(avail_actions)\n",
    "    #)\n",
    "    \n",
    "    unavail_actions = 1 - avail_actions_batchified\n",
    "    q_vals = q_vals - (unavail_actions * 1e10)\n",
    "    actions_array = jnp.argmax(q_vals, axis=-1)\n",
    "    actions_dict = unbatchify(actions_array)\n",
    "    print(\"actions\", actions_dict)\n",
    "\n",
    "\n",
    "    new_obs, new_env_state, rewards, dones, infos = wrapped_env.batch_step(\n",
    "        rng_s, env_state, actions_dict\n",
    "    )\n",
    "    \n",
    "    timestep = Timestep(\n",
    "        obs=last_obs,\n",
    "        actions=actions_dict,\n",
    "        rewards=jax.tree.map(lambda x:config.get(\"REW_SCALE\", 1)*x, rewards),\n",
    "        dones=last_dones,\n",
    "        avail_actions=avail_actions,\n",
    "    )\n",
    "\n",
    "    #per_agent_dones = jnp.stack(\n",
    "    #    [dones[agent] for agent in env.agents], axis=-1\n",
    "    #)  # shape: [num_envs, num_agents]\n",
    "\n",
    "    transition = Transition(\n",
    "        jnp.tile(dones[\"__all__\"], env.num_agents),\n",
    "        #last_dones,\n",
    "        #actions_array.squeeze(),\n",
    "        batchify(rewards).reshape(-1),#.reshape(batch_size, config[\"NUM_ENVS\"], -1),  #batchify(rewards, env.agents, config[\"NUM_ACTORS\"]).squeeze(),\n",
    "        #_obs,\n",
    "        env_state.env_state.state,\n",
    "        #infos,\n",
    "        #avail_actions,\n",
    "    )\n",
    "\n",
    "    runner_state = (policy_params, new_env_state, new_obs, dones, new_hstate, rng)\n",
    "    \n",
    "    return runner_state, (transition, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15925bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unroll(\n",
    "    runner_state,\n",
    "    episode_length: int,\n",
    "    play_step_fn: Callable[\n",
    "        [EnvState, Params, RNGKey],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            Params,\n",
    "            RNGKey,\n",
    "            Transition,\n",
    "        ],\n",
    "    ],\n",
    ") -> Tuple[EnvState, Transition]:\n",
    "    \"\"\"Generates an episode according to the agent's policy, returns the final state of\n",
    "    the episode and the transitions of the episode.\n",
    "\n",
    "    Args:\n",
    "        init_state: first state of the rollout.\n",
    "        policy_params: params of the individual.\n",
    "        key: random key for stochasiticity handling.\n",
    "        episode_length: length of the rollout.\n",
    "        play_step_fn: function describing how a step need to be taken.\n",
    "\n",
    "    Returns:\n",
    "        A new state, the experienced transition.\n",
    "    \"\"\"\n",
    "\n",
    "    def _scan_play_step_fn(\n",
    "        carry, unused_arg: Any #: Tuple[EnvState, Params, RNGKey]\n",
    "    ) -> Tuple[Tuple[EnvState, Params, RNGKey], Transition]:\n",
    "        runner_state, (transitions, timestep) = play_step_fn(carry)\n",
    "        return runner_state, (transitions, timestep)\n",
    "\n",
    "    runner_state, (transitions, timestep) = jax.lax.scan(\n",
    "        _scan_play_step_fn,\n",
    "        runner_state,\n",
    "        (),\n",
    "        length=episode_length,\n",
    "    )\n",
    "    return runner_state, (transitions, timestep)\n",
    "\n",
    "\n",
    "def get_mask_from_transitions(\n",
    "    data: Transition,\n",
    ") -> jnp.ndarray:\n",
    "    is_done = jnp.clip(jnp.cumsum(data.global_done, axis=1), 0, 1)\n",
    "    mask = jnp.roll(is_done, 1, axis=1)\n",
    "    mask = mask.at[:, 0].set(0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b871dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_function(\n",
    "    policy_params: Genotype,\n",
    "    key: RNGKey,\n",
    "    episode_length: int,\n",
    "    play_reset_fn: Callable[[RNGKey], EnvState],\n",
    "    play_step_fn: Callable[\n",
    "        [EnvState, Params, RNGKey], Tuple[EnvState, Params, RNGKey, QDTransition]\n",
    "    ],\n",
    "    descriptor_extractor: Callable[[QDTransition, jnp.ndarray], Descriptor],\n",
    ") -> Tuple[Fitness, Descriptor, ExtraScores]:\n",
    "    \"\"\"Evaluates policies contained in policies_params in parallel.\n",
    "    The play_reset_fn function allows for a more general scoring_function that can be\n",
    "    called with different batch-size and not only with a batch-size of the same\n",
    "    dimension as init_states.\n",
    "\n",
    "    To define purely stochastic environments, using the reset function from the\n",
    "    environment, use \"play_reset_fn = env.reset\".\n",
    "\n",
    "    To define purely deterministic environments, as in \"scoring_function\", generate\n",
    "    a single init_state using \"init_state = env.reset(key)\", then use\n",
    "    \"play_reset_fn = lambda key: init_state\".\n",
    "\n",
    "    Args:\n",
    "        policies_params: The parameters of closed-loop controllers/policies to evaluate.\n",
    "        key: A jax random key\n",
    "        episode_length: The maximal rollout length.\n",
    "        play_reset_fn: The function to reset the environment and obtain initial states.\n",
    "        play_step_fn: The function to play a step of the environment.\n",
    "        descriptor_extractor: The function to extract the descriptor.\n",
    "\n",
    "    Returns:\n",
    "        fitness: Array of fitnesses of all evaluated policies\n",
    "        descriptor: Behavioural descriptors of all evaluated policies\n",
    "        extra_scores: Additional information resulting from the evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # Reset environments\n",
    "    key, subkey = jax.random.split(key)\n",
    "    #keys = jax.random.split(subkey, jax.tree.leaves(policies_params)[0].shape[0])\n",
    "    #keys = jax.random.split(subkey, config[\"NUM_ENVS\"])\n",
    "    #init_obs, init_env_state = jax.vmap(play_reset_fn, in_axes=(0,))(keys)\n",
    "    init_obs, init_env_state = play_reset_fn(key)\n",
    "    #print(\"init_obs is a dict with keys:\", init_obs.keys())\n",
    "    #for k, v in init_obs.items():\n",
    "    #    print(f\"init_obs[{k}] shape:\", v.shape) \n",
    "    \n",
    "    keys = jax.random.split(key, batch_size)\n",
    "\n",
    "    #init_dones = {\n",
    "    #    agent: jnp.zeros((config[\"NUM_ENVS\"]), dtype=bool)\n",
    "    #    for agent in env.agents + [\"__all__\"]\n",
    "    #}\n",
    "    init_hstate = ScannedRNN.initialize_carry(\n",
    "        config[\"HIDDEN_SIZE\"], len(env.agents), config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    \n",
    "    batched_env_state = jax.tree.map(lambda x: jnp.stack([x] * batch_size), init_env_state)\n",
    "    batched_obs = jax.tree.map(lambda x: jnp.stack([x] * batch_size), init_obs)\n",
    "    batched_dones = {\n",
    "        agent: jnp.stack([jnp.zeros(config[\"NUM_ENVS\"], dtype=bool)] * batch_size)\n",
    "        for agent in env.agents + [\"__all__\"]\n",
    "    }\n",
    "    batched_hstate = jnp.stack([init_hstate] * batch_size)\n",
    "\n",
    "    #print(\"batched_obs is a dict with keys:\", batched_obs.keys())\n",
    "    #for k, v in batched_obs.items():\n",
    "    #    print(f\"batched_obs[{k}] shape:\", v.shape) \n",
    "\n",
    "    init_runner_state = (policy_params, batched_env_state, batched_obs, batched_dones, batched_hstate, keys)\n",
    "    #print(\"init_runner_state:\")\n",
    "    #jax.tree.map(lambda x: print(x.shape), init_runner_state)\n",
    "    \n",
    "    # Step environments\n",
    "    unroll_fn = functools.partial(\n",
    "        generate_unroll,\n",
    "        episode_length=episode_length,\n",
    "        play_step_fn=play_step_fn,\n",
    "    )\n",
    "    #keys = jax.random.split(key, jax.tree.leaves(policies_params)[0].shape[0])\n",
    "    _, (data, timestep) = jax.vmap(unroll_fn)(init_runner_state) # data = Transistions data struc\n",
    "    #jax.debug.print(\"after vmap timestep shape: {}\", \n",
    "    #jax.tree.map(lambda x: x.shape, timestep))\n",
    "\n",
    "    # Create a mask to extract data properly\n",
    "    mask = get_mask_from_transitions(data)\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"data.reward shape:\", data.reward.shape)\n",
    "    print(\"data.global_done shape:\", data.global_done.shape)\n",
    "    print(\"mask shape:\", mask.shape)\n",
    "    #fitnesses = jnp.sum(data.reward * (1.0 - mask), axis=1)\n",
    "    fitnesses = jnp.sum(data.reward * (1.0 - mask), axis=(1, 2))  # shape (32,)\n",
    "    fitnesses = fitnesses.reshape(-1, 1)  # shape (32, 1)\n",
    "    descriptors = descriptor_extractor(data, mask)\n",
    "\n",
    "    print(\"fitness shape:\", fitnesses.shape)\n",
    "    print(\"descriptors shape:\", descriptors.shape)\n",
    "\n",
    "    return fitnesses, descriptors, {\"transitions\": timestep}#data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b2edde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptors(data):\n",
    "    \"\"\"\n",
    "    Compute per-trajectory descriptors:\n",
    "    - % of units that moved last step (per batch item)\n",
    "    - % of units that attacked last step (per batch item)\n",
    "    - Avg ally-ally distance at final step (per batch item)\n",
    "    \n",
    "    Output: shape (batch_size, 3)\n",
    "    \"\"\"\n",
    "    movement_actions = data.env_state.prev_movement_actions          # (B, T, E, A, 2)\n",
    "    attack_actions = data.env_state.prev_attack_actions              # (B, T, E, A)\n",
    "    alive = data.env_state.unit_alive.astype(jnp.float32)            # (B, T, E, A)\n",
    "    positions = data.env_state.unit_positions                        # (B, T, E, A, 2)\n",
    "    teams = data.env_state.unit_teams                                # (B, T, E, A)\n",
    "\n",
    "    moved = jnp.any(movement_actions != 0.0, axis=-1)                # (B, T, E, A)\n",
    "    attacked = attack_actions > 0                                    # (B, T, E, A)\n",
    "    is_ally = (teams == 0).astype(jnp.float32)                        # (B, T, E, A)\n",
    "    ally_alive = alive * is_ally                                      # (B, T, E, A)\n",
    "\n",
    "    # Per batch movement/attack % (sum over all but batch dim)\n",
    "    def pct_nonzero(mask, alive_mask):\n",
    "        numerator = jnp.sum(mask * alive_mask, axis=(1, 2, 3))        # (B,)\n",
    "        denominator = jnp.sum(alive_mask, axis=(1, 2, 3)) + 1e-8      # (B,)\n",
    "        return numerator / denominator                                # (B,)\n",
    "\n",
    "    movement_pct = pct_nonzero(moved, ally_alive)                    # (B,)\n",
    "    attack_pct = pct_nonzero(attacked, ally_alive)                   # (B,)\n",
    "\n",
    "    # Final step only for positions and masks\n",
    "    final_positions = positions[:, -1, :, :, :]                      # (B, E, A, 2)\n",
    "    final_alive = alive[:, -1, :, :]                                 # (B, E, A)\n",
    "    final_teams = teams[:, -1, :, :]                                 # (B, E, A)\n",
    "    ally_mask = (final_teams == 0) * final_alive                     # (B, E, A)\n",
    "\n",
    "    def avg_pairwise_distance(pos, mask):\n",
    "        \"\"\"\n",
    "        pos: (A, 2)\n",
    "        mask: (A,)\n",
    "        returns: scalar distance\n",
    "        \"\"\"\n",
    "        diffs = pos[:, None, :] - pos[None, :, :]                    # (A, A, 2)\n",
    "        dists = jnp.linalg.norm(diffs, axis=-1)                      # (A, A)\n",
    "        pair_mask = mask[:, None] * mask[None, :]                    # (A, A)\n",
    "        not_self = 1 - jnp.eye(mask.shape[0])\n",
    "        valid_pairs = pair_mask * not_self\n",
    "        total_dist = jnp.sum(dists * valid_pairs)\n",
    "        num_pairs = jnp.sum(valid_pairs)\n",
    "        return jnp.where(num_pairs > 0, total_dist / num_pairs, 0.0)\n",
    "\n",
    "    # vmap over batch and envs: returns shape (B, E)\n",
    "    avg_dists = jax.vmap(\n",
    "        jax.vmap(avg_pairwise_distance, in_axes=(0, 0)),\n",
    "        in_axes=(0, 0)\n",
    "    )(final_positions, ally_mask)\n",
    "\n",
    "    avg_dist = jnp.mean(avg_dists, axis=1)  # mean over envs → shape (B,)\n",
    "\n",
    "    # Normalize and clip\n",
    "    max_distance = 32\n",
    "    quarter_max = 0.25 * max_distance \n",
    "    norm_dist = avg_dist / quarter_max\n",
    "    norm_dist = jnp.clip(norm_dist, 0.0, 1.0)\n",
    "\n",
    "    # Stack all descriptors → shape (B, 3)\n",
    "    #return jnp.stack([movement_pct, attack_pct, norm_dist], axis=-1)\n",
    "    return jnp.stack([movement_pct, norm_dist], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "def descriptor_extraction_fn(data: Transition, mask: jnp.ndarray) -> Descriptor:\n",
    "    \"\"\"Compute final xy position.\n",
    "\n",
    "    This function suppose that state descriptor is the xy position, as it\n",
    "    just select the final one of the state descriptors given.\n",
    "    \"\"\"\n",
    "    # reshape mask for descriptor extraction\n",
    "    mask = jnp.expand_dims(mask, axis=-1)\n",
    "\n",
    "    # Get descriptor\n",
    "    #last_index = jnp.int32(jnp.sum(1.0 - mask, axis=1)) - 1\n",
    "    #descriptors = jax.vmap(lambda x, y: x[y])(data.state_desc, last_index)\n",
    "    descriptors = compute_descriptors(data)\n",
    "\n",
    "    # remove the dim coming from the trajectory\n",
    "    return descriptors#.squeeze(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b33c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from qdax.core.emitters.repertoire_selectors.selector import Selector\n",
    "from qdax.core.emitters.qpg_emitter import QualityPGConfig, QualityPGEmitterState\n",
    "from qdax.tasks.brax.v1.envs.base_env import QDEnv\n",
    "\n",
    "\n",
    "from qdax.core.emitters.multi_emitter import MultiEmitter\n",
    "from qdax.core.containers.ga_repertoire import GARepertoire\n",
    "from qdax.core.containers.repertoire import Repertoire\n",
    "#from qdax.core.neuroevolution.losses.td3_loss import make_td3_loss_fn\n",
    "from qdax.core.emitters.emitter import Emitter\n",
    "from qdax.core.neuroevolution.networks.networks import QModule\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition, ReplayBuffer\n",
    "\n",
    "from qdax.core.emitters.pga_me_emitter import PGAMEConfig, PGAMEEmitter\n",
    "\n",
    "import flashbax as fbx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5eece881",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_iql_loss_fn(\n",
    "        policy_fn: Callable[[Params, Observation], jnp.ndarray],\n",
    "        reward_scaling: float,\n",
    "        discount: float,\n",
    ") -> Callable[[Params, Transition], jnp.ndarray]:\n",
    "    \"\"\"Creates the loss functions for IQL.\n",
    "\n",
    "    Args:\n",
    "        policy_fn: forward pass through the neural network defining the policy.\n",
    "        reward_scaling: value to multiply the reward given by the environment.\n",
    "        discount: discount factor.\n",
    "\n",
    "    Returns:\n",
    "        Return the loss functions used to train the policy.\n",
    "    \"\"\"\n",
    "    def _policy_loss_fn(\n",
    "        policy_params: Params,\n",
    "        target_policy_params: Params,\n",
    "        minibatch: Transition,\n",
    "        #emitter_state: CustomQualityPGEmitterState,\n",
    "    ) -> jnp.ndarray:\n",
    "        #minibatch = emitter_state.buffer.sample(emitter_state.buffer_state, emitter_state.key).experience\n",
    "\n",
    "        minibatch = jax.tree.map(\n",
    "            lambda x: jnp.swapaxes(\n",
    "                x[:, 0], 0, 1\n",
    "            ),  # remove the dummy sequence dim (1) and swap batch and temporal dims\n",
    "            minibatch,\n",
    "        )  # (max_time_steps, batch_size, ...)\n",
    "\n",
    "        # preprocess network input\n",
    "        init_hs = ScannedRNN.initialize_carry(\n",
    "            config[\"HIDDEN_SIZE\"],\n",
    "            len(env.agents),\n",
    "            config[\"BUFFER_BATCH_SIZE\"],\n",
    "        )\n",
    "        # num_agents, timesteps, batch_size, ...\n",
    "        _obs = batchify(minibatch.obs)\n",
    "        _dones = batchify(minibatch.dones)\n",
    "        _actions = batchify(minibatch.actions)\n",
    "        _rewards = batchify(minibatch.rewards)\n",
    "        _avail_actions = batchify(minibatch.avail_actions)\n",
    "        ###################################################################################################\n",
    "        #print(\"_obs.shape\", _obs.shape)\n",
    "        #print(\"_does.shape\", _dones.shape)\n",
    "        #print(\"init_hs.shape\", init_hs.shape)\n",
    "        ###################################################################################################\n",
    "        _, q_next_target = jax.vmap(policy_fn, in_axes=(None, 0, 0, 0))(\n",
    "            target_policy_params, #train_state.target_network_params,\n",
    "            init_hs,\n",
    "            _obs,\n",
    "            _dones,\n",
    "        )  # (num_agents, timesteps, batch_size, num_actions)\n",
    "\n",
    "        _, q_vals = jax.vmap(policy_fn, in_axes=(None, 0, 0, 0))(\n",
    "            policy_params,\n",
    "            init_hs,\n",
    "            _obs,\n",
    "            _dones,\n",
    "        )  # (num_agents, timesteps, batch_size, num_actions)\n",
    "\n",
    "        # get logits of the chosen actions\n",
    "        chosen_action_q_vals = jnp.take_along_axis(\n",
    "            q_vals,\n",
    "            _actions[..., np.newaxis],\n",
    "            axis=-1,\n",
    "        ).squeeze(\n",
    "            -1\n",
    "        )  # (num_agents, timesteps, batch_size,)\n",
    "\n",
    "        unavailable_actions = 1 - _avail_actions\n",
    "        valid_q_vals = q_vals - (unavailable_actions * 1e10)\n",
    "\n",
    "        # get the q values of the next state\n",
    "        q_next = jnp.take_along_axis(\n",
    "            q_next_target,\n",
    "            jnp.argmax(valid_q_vals, axis=-1)[..., np.newaxis],\n",
    "            axis=-1,\n",
    "        ).squeeze(\n",
    "            -1\n",
    "        )  # (num_agents, timesteps, batch_size,)\n",
    "\n",
    "        target = (\n",
    "            _rewards[:, :-1] * reward_scaling\n",
    "            + (1 - _dones[:, :-1]) * discount * q_next[:, 1:]\n",
    "        )\n",
    "\n",
    "        chosen_action_q_vals = chosen_action_q_vals[:, :-1]\n",
    "        loss = jnp.mean(\n",
    "            (chosen_action_q_vals - jax.lax.stop_gradient(target)) ** 2\n",
    "        )\n",
    "\n",
    "        #return loss, chosen_action_q_vals.mean()\n",
    "        return loss\n",
    "    \n",
    "    return _policy_loss_fn\n",
    "\n",
    "#class CustomQualityPGEmitterState(QualityPGEmitterState):\n",
    "    #target_network_params: Any\n",
    "#    buffer_state: Any\n",
    "\n",
    "@chex.dataclass(frozen=True)\n",
    "class CustomQualityPGEmitterState:\n",
    "    #target_network_params: Any\n",
    "    buffer_state: Any\n",
    "    #replay_buffer: Any = struct.field(pytree_node=False)  # Static Python object\n",
    "    key: RNGKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3954d06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myQualityPGEmitter(Emitter):\n",
    "    \"\"\"\n",
    "    A policy gradient emitter used to implement the Policy Gradient Assisted MAP-Elites\n",
    "    (PGA-Map-Elites) algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: QualityPGConfig,\n",
    "        policy_network: nn.Module,\n",
    "        env: QDEnv,\n",
    "        selector: Optional[Selector] = None,\n",
    "    ) -> None:\n",
    "        self._config = config\n",
    "        self._env = env\n",
    "        self._selector = selector\n",
    "        self._actor_critic_iterations = int(\n",
    "            config.num_critic_training_steps / config.policy_delay\n",
    "        )  # actor and critic training are packed into a single function\n",
    "\n",
    "        # Init Critics\n",
    "        #critic_network = QModule(\n",
    "        #    n_critics=2, hidden_layer_sizes=self._config.critic_hidden_layer_size\n",
    "        #)\n",
    "        #critic_network = CriticRNN(action_dim=env.action_space(env.agents[0]).n, config=config, n_critics=2)\n",
    "        #target_q_network = RNNQNetwork(\n",
    "        #    action_dim=wrapped_env.max_action_space,\n",
    "        #    hidden_dim=config[\"HIDDEN_SIZE\"],\n",
    "        #)\n",
    "        \n",
    "        #self._critic_network = target_q_network #critic_network\n",
    "\n",
    "        # Set up the losses and optimizers - return the opt states\n",
    "        self._policy_loss_fn = make_iql_loss_fn( #make_td3_loss_fn(\n",
    "            policy_fn=policy_network.apply,\n",
    "            #critic_fn=critic_network.apply,\n",
    "            reward_scaling=self._config.reward_scaling,\n",
    "            discount=self._config.discount,\n",
    "            #noise_clip=self._config.noise_clip,\n",
    "            #policy_noise=self._config.policy_noise,\n",
    "        )\n",
    "\n",
    "        # Init optimizers\n",
    "        #self._actor_optimizer = optax.adam(\n",
    "        #    learning_rate=self._config.actor_learning_rate\n",
    "        #)\n",
    "        #self._critic_optimizer = optax.adam(\n",
    "        #    learning_rate=self._config.critic_learning_rate\n",
    "        #)\n",
    "        #self._policies_optimizer = optax.adam(\n",
    "        #    learning_rate=self._config.policy_learning_rate\n",
    "        #)\n",
    "        \n",
    "        #lr_scheduler = optax.linear_schedule(\n",
    "        #    init_value=config[\"LR\"],\n",
    "        #    end_value=1e-10,\n",
    "        #    transition_steps=(config[\"NUM_EPOCHS\"]) * config[\"NUM_UPDATES\"],\n",
    "        #)\n",
    "        #lr = lr_scheduler if config.get(\"LR_LINEAR_DECAY\", False) else config[\"LR\"]\n",
    "        #tx = optax.chain(\n",
    "        #    optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "        #    optax.radam(learning_rate=lr),\n",
    "        #)\n",
    "        #self._policies_optimizer = tx\n",
    "\n",
    "        buffer = fbx.make_trajectory_buffer(\n",
    "            max_length_time_axis=self._config.replay_buffer_size // batch_size,\n",
    "            min_length_time_axis=self._config.batch_size,\n",
    "            sample_batch_size=self._config.batch_size,\n",
    "            add_batch_size=self._config.num_envs * batch_size,\n",
    "            sample_sequence_length=1,\n",
    "            period=1,\n",
    "        )\n",
    "        self._buffer = buffer\n",
    "\n",
    "        # Init optimizers\n",
    "        self._policies_optimizer = optax.adam(\n",
    "            learning_rate=self._config.policy_learning_rate\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def batch_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            the batch size emitted by the emitter.\n",
    "        \"\"\"\n",
    "        return self._config.env_batch_size\n",
    "\n",
    "    @property\n",
    "    def use_all_data(self) -> bool:\n",
    "        \"\"\"Whether to use all data or not when used along other emitters.\n",
    "\n",
    "        QualityPGEmitter uses the transitions from the genotypes that were generated\n",
    "        by other emitters.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def init(\n",
    "        self,\n",
    "        key: RNGKey,\n",
    "        repertoire: Repertoire,\n",
    "        genotypes: Genotype,\n",
    "        fitnesses: Fitness,\n",
    "        descriptors: Descriptor,\n",
    "        extra_scores: ExtraScores,\n",
    "    ) -> CustomQualityPGEmitterState:\n",
    "        \"\"\"Initializes the emitter state.\n",
    "\n",
    "        Args:\n",
    "            genotypes: The initial population.\n",
    "            key: A random key.\n",
    "\n",
    "        Returns:\n",
    "            The initial state of the PGAMEEmitter.\n",
    "        \"\"\"\n",
    "        '''\n",
    "        #observation_size = self._env.observation_size\n",
    "        #action_size = self._env.action_size\n",
    "        #descriptor_size = self._env.state_descriptor_length\n",
    "\n",
    "        obs_space = self._env.observation_space(env.agents[0])#.shape[0]\n",
    "        action_space = self._env.action_space(env.agents[0])#.shape[0]\n",
    "\n",
    "        observation_size = int(np.prod(obs_space.shape))\n",
    "        action_size = int(np.prod(action_space.shape))\n",
    "        descriptor_size = 2 # hardcoded\n",
    "\n",
    "        # Initialise critic, greedy actor and population\n",
    "        #key, subkey = jax.random.split(key)\n",
    "        #fake_obs = jnp.zeros(shape=(observation_size,))\n",
    "        #fake_action = jnp.zeros(shape=(action_size,))\n",
    "        #critic_params = self._critic_network.init(\n",
    "        #    subkey, obs=fake_obs, actions=fake_action\n",
    "        #)\n",
    "\n",
    "        #init_hstate = ScannedRNN.initialize_carry(1, config[\"GRU_HIDDEN_DIM\"])\n",
    "        key, subkey = jax.random.split(key)\n",
    "        #fake_hstate = jnp.stack([init_hstate] * 1) \n",
    "        #fake_obs = tuple(jnp.stack([x] * batch_size) for x in init_x)\n",
    "        #fake_action = jnp.zeros(shape=(action_size,))\n",
    "        #critic_params = self._critic_network.init(subkey, fake_hstate, fake_obs, fake_action)\n",
    "\n",
    "        fake_x = (\n",
    "            jnp.zeros(\n",
    "                (1, config[\"NUM_ENVS\"], env.observation_space(env.agents[0]).shape[0])\n",
    "            ),\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"])),\n",
    "            jnp.zeros((1, config[\"NUM_ENVS\"], env.action_space(env.agents[0]).n)),\n",
    "        )\n",
    "        fake_hstate = ScannedRNN.initialize_carry(config[\"NUM_ENVS\"], config[\"GRU_HIDDEN_DIM\"])\n",
    "        fake_action = jnp.zeros(shape=(action_size,))\n",
    "        critic_params = self._critic_network.init(subkey, fake_hstate, fake_x, fake_action)\n",
    "\n",
    "        #target_critic_params = jax.tree.map(lambda x: x, critic_params)\n",
    "\n",
    "        #actor_params = jax.tree.map(lambda x: x[0], genotypes)\n",
    "        #target_actor_params = jax.tree.map(lambda x: x[0], genotypes)\n",
    "\n",
    "        # Prepare init optimizer states\n",
    "        #critic_optimizer_state = self._critic_optimizer.init(critic_params)\n",
    "        #actor_optimizer_state = self._actor_optimizer.init(actor_params)\n",
    "\n",
    "        # Initialize replay buffer\n",
    "        dummy_transition = QDTransition.init_dummy(\n",
    "            observation_dim=observation_size,\n",
    "            action_dim=action_size,\n",
    "            descriptor_dim=descriptor_size,\n",
    "        )\n",
    "\n",
    "        replay_buffer = ReplayBuffer.init(\n",
    "            buffer_size=self._config.replay_buffer_size, transition=dummy_transition\n",
    "        )\n",
    "\n",
    "        # get the transitions out of the dictionary\n",
    "        assert \"transitions\" in extra_scores.keys(), \"Missing transitions or wrong key\"\n",
    "        transitions = extra_scores[\"transitions\"]\n",
    "\n",
    "        #print(\"Transitions type:\", type(transitions))\n",
    "        #print(\"Transitions fields:\", transitions)\n",
    "\n",
    "        # add transitions in the replay buffer\n",
    "        replay_buffer = replay_buffer.insert(transitions)\n",
    "\n",
    "        '''\n",
    "\n",
    "        ###########################################################################################################################################\n",
    "        '''\n",
    "        init_x = (\n",
    "            jnp.zeros(\n",
    "                (1, 1, wrapped_env.obs_size)\n",
    "            ),  # (time_step, batch_size, obs_size)\n",
    "            jnp.zeros((1, 1)),  # (time_step, batch size)\n",
    "        )\n",
    "        init_hs = ScannedRNN.initialize_carry(\n",
    "            config[\"HIDDEN_SIZE\"], 1\n",
    "        )  # (batch_size, hidden_dim)\n",
    "        network_params = policy_network.init(rng, init_hs, *init_x)\n",
    "\n",
    "        lr_scheduler = optax.linear_schedule(\n",
    "            init_value=config[\"LR\"],\n",
    "            end_value=1e-10,\n",
    "            transition_steps=(config[\"NUM_EPOCHS\"]) * config[\"NUM_UPDATES\"],\n",
    "        )\n",
    "\n",
    "        lr = lr_scheduler if config.get(\"LR_LINEAR_DECAY\", False) else config[\"LR\"]\n",
    "\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.radam(learning_rate=lr),\n",
    "        )\n",
    "\n",
    "        train_state = CustomTrainState.create(\n",
    "            apply_fn=policy_network.apply,\n",
    "            params=network_params,\n",
    "            target_network_params=network_params,\n",
    "            tx=tx,\n",
    "        )\n",
    "        '''\n",
    "        # INIT BUFFER\n",
    "        # to initalize the buffer is necessary to sample a trajectory to know its strucutre\n",
    "        def _env_sample_step(env_state, unused):\n",
    "            rng, key_a, key_s = jax.random.split(\n",
    "                jax.random.PRNGKey(0), 3\n",
    "            )  # use a dummy rng here\n",
    "            key_a = jax.random.split(key_a, env.num_agents)\n",
    "            actions = {\n",
    "                agent: self._env.batch_sample(key_a[i], agent)\n",
    "                for i, agent in enumerate(env.agents)\n",
    "            }\n",
    "            avail_actions = self._env.get_valid_actions(env_state.env_state)\n",
    "            obs, env_state, rewards, dones, infos = self._env.batch_step(\n",
    "                key_s, env_state, actions\n",
    "            )\n",
    "            timestep = Timestep(\n",
    "                obs=obs,\n",
    "                actions=actions,\n",
    "                rewards=rewards,\n",
    "                dones=dones,\n",
    "                avail_actions=avail_actions,\n",
    "            )\n",
    "            return env_state, timestep\n",
    "\n",
    "        _, _env_state = self._env.batch_reset(rng)\n",
    "        _, sample_traj = jax.lax.scan(\n",
    "            _env_sample_step, _env_state, None, self._config.num_steps\n",
    "        )\n",
    "        sample_traj_unbatched = jax.tree.map(\n",
    "            lambda x: x[:, 0], sample_traj\n",
    "        )  # remove the NUM_ENV dim\n",
    "        \n",
    "        # Remove time dimension\n",
    "        sample_traj_unbatched = jax.tree.map(lambda x: x[:, 0], sample_traj)  # (16, …)\n",
    "        # Tile across parallel_policies(batch_size) to match real add_batch_size\n",
    "        #sample_traj_unbatched = jax.tree.map(\n",
    "        #    lambda x: jnp.repeat(x, batch_size, axis=0),\n",
    "        #    sample_traj_unbatched\n",
    "        #)  # now (128, …)\n",
    "        \n",
    "        print(\"Init sample_traj shape:\", jax.tree.map(lambda x: x.shape, sample_traj_unbatched))\n",
    "        buffer_state = self._buffer.init(sample_traj_unbatched)\n",
    "        \n",
    "        # get the transitions out of the dictionary\n",
    "        assert \"transitions\" in extra_scores.keys(), \"Missing transitions or wrong key\"\n",
    "        transitions = extra_scores[\"transitions\"]\n",
    "\n",
    "        # add transitions in the replay buffer\n",
    "        #replay_buffer = replay_buffer.insert(transitions)\n",
    "        # BUFFER UPDATE\n",
    "        #print(\"Transitions keys:\", transitions.keys())\n",
    "        #for k, v in transitions.items():\n",
    "        #    print(k, v.shape, v.dtype)\n",
    "        \n",
    "        #jax.tree.map(lambda x: print(x.shape), transitions)\n",
    "        #jax.tree.map(lambda x: print(\"Buffer expects:\", x.shape), buffer_state.experience)\n",
    "        \n",
    "        def prepare_for_buffer(x):\n",
    "            # x: (P, T, E, *feat_dims)\n",
    "            P, T, E, *feat_dims = x.shape\n",
    "\n",
    "            # First move to (P, E, T, *feat_dims)\n",
    "            x = x.transpose(0, 2, 1, *range(3, x.ndim))\n",
    "\n",
    "            # Merge P * E into batch\n",
    "            x = x.reshape(P * E, T, *feat_dims)\n",
    "\n",
    "            # Add dummy seq_len=1 in the middle -> (batch, 1, T, feat_dim...)\n",
    "            x = x[:, np.newaxis, ...]\n",
    "\n",
    "            return x\n",
    "        \n",
    "        buffer_traj_batch = jax.tree.map(\n",
    "            prepare_for_buffer,\n",
    "            transitions\n",
    "        )\n",
    "        #buffer_traj_batch = jax.tree.map(\n",
    "        #    lambda x: jnp.swapaxes(x, 0, 1)[\n",
    "        #        :, np.newaxis\n",
    "        #    ],  # put the batch dim first and add a dummy sequence dim\n",
    "        #    transitions,\n",
    "        #)  # (num_envs, 1, time_steps, ...)\n",
    "        #buffer_traj_batch = jax.tree.map(\n",
    "        #    lambda x: jnp.swapaxes(\n",
    "        #        # Reorder from (num_policies, time_steps, num_envs, …)\n",
    "        #        # → (time_steps, num_policies, num_envs, …)\n",
    "        #        x.transpose(1, 0, 2, *range(3, x.ndim))\n",
    "        #        # Merge num_policies * num_envs into one dimension\n",
    "        #        .reshape(x.shape[1], -1, *x.shape[3:]),\n",
    "        #        0, 1  # swap (time_steps, merged_envs, …) → (merged_envs, time_steps, …)\n",
    "        #    )[:, np.newaxis],  # add dummy sequence dim\n",
    "        #    transitions,\n",
    "        #)\n",
    "        print(\"Runtime sample_traj shape:\", jax.tree.map(lambda x: x.shape, buffer_traj_batch))\n",
    "\n",
    "        buffer_state = self._buffer.add(buffer_state, buffer_traj_batch)\n",
    "\n",
    "\n",
    "        # Initial training state\n",
    "        #emitter_state = QualityPGEmitterState(\n",
    "        #    critic_params=critic_params,\n",
    "        #    critic_optimizer_state=critic_optimizer_state,\n",
    "        #    actor_params=actor_params,\n",
    "        #    actor_opt_state=actor_optimizer_state,\n",
    "        #    target_critic_params=target_critic_params,\n",
    "        #    target_actor_params=target_actor_params,\n",
    "        #    replay_buffer=replay_buffer,\n",
    "        #    key=key,\n",
    "        #)\n",
    "\n",
    "        # Initial training state\n",
    "        emitter_state = CustomQualityPGEmitterState(\n",
    "            #critic_params=None, #critic_params,\n",
    "            #critic_optimizer_state=None, #critic_optimizer_state,\n",
    "            #actor_params=None, #actor_params,\n",
    "            #actor_opt_state=None, #actor_optimizer_state,\n",
    "            #target_critic_params=None, #target_critic_params,\n",
    "            #target_actor_params=None, #target_actor_params,\n",
    "            #replay_buffer=buffer,\n",
    "            buffer_state=buffer_state,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "        return emitter_state\n",
    "\n",
    "    def emit(  # type: ignore\n",
    "        self,\n",
    "        repertoire: GARepertoire,\n",
    "        emitter_state: CustomQualityPGEmitterState,\n",
    "        key: RNGKey,\n",
    "    ) -> Tuple[Genotype, ExtraScores]:\n",
    "        \"\"\"Do a step of PG emission.\n",
    "\n",
    "        Args:\n",
    "            repertoire: the current repertoire of genotypes\n",
    "            emitter_state: the state of the emitter used\n",
    "            key: a random key\n",
    "\n",
    "        Returns:\n",
    "            A batch of offspring, a empty dict for signature.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = self._config.env_batch_size\n",
    "\n",
    "        # sample parents\n",
    "        mutation_pg_batch_size = int(batch_size) #int(batch_size - 1)\n",
    "        parents = repertoire.select(\n",
    "            key, mutation_pg_batch_size, selector=self._selector\n",
    "        ).genotypes\n",
    "\n",
    "        # apply the pg mutation\n",
    "        offsprings_pg = self.emit_pg(emitter_state, parents)\n",
    "\n",
    "        # get the actor (greedy actor)\n",
    "        #offspring_actor = self.emit_actor(emitter_state)\n",
    "\n",
    "        # add dimension for concatenation\n",
    "        #offspring_actor = jax.tree.map(\n",
    "        #    lambda x: jnp.expand_dims(x, axis=0), offspring_actor\n",
    "        #)\n",
    "        # gather offspring\n",
    "        #genotypes = jax.tree.map(\n",
    "        #    lambda x, y: jnp.concatenate([x, y], axis=0),\n",
    "        #    offsprings_pg,\n",
    "        #    offspring_actor,\n",
    "        #)\n",
    "        genotypes = offsprings_pg\n",
    "\n",
    "        return genotypes, {}\n",
    "\n",
    "    def emit_pg(\n",
    "        self, emitter_state: CustomQualityPGEmitterState, parents: Genotype\n",
    "    ) -> Genotype:\n",
    "        \"\"\"Emit the offsprings generated through pg mutation.\n",
    "\n",
    "        Args:\n",
    "            emitter_state: current emitter state, contains critic and\n",
    "                replay buffer.\n",
    "            parents: the parents selected to be applied gradients in order\n",
    "                to mutate towards better performance.\n",
    "\n",
    "        Returns:\n",
    "            A new set of offsprings.\n",
    "        \"\"\"\n",
    "\n",
    "        # create a batch of policy optimizer states\n",
    "        policy_opt_states = jax.vmap(self._policies_optimizer.init)(parents)\n",
    "\n",
    "        # prepare the batched policy update function with vmapping\n",
    "        batched_policy_update_fn = jax.vmap(\n",
    "            partial(self._update_policy, emitter_state=emitter_state), #critic_params=emitter_state.critic_params),\n",
    "            in_axes=(0, 0)#, None),\n",
    "        )\n",
    "\n",
    "        def scan_update_policies(\n",
    "            carry: Tuple[Params, optax.OptState, RNGKey],\n",
    "            _: None,\n",
    "        ) -> Tuple[Tuple[Params, optax.OptState, RNGKey], Any]:\n",
    "\n",
    "            # Unpack the carry\n",
    "            (policy_params, policy_opt_state, key) = carry\n",
    "            key, subkey = jax.random.split(key)\n",
    "\n",
    "            # sample a mini-batch of data from the replay-buffer\n",
    "            #transitions = emitter_state.replay_buffer.sample(\n",
    "            #    subkey, self._config.batch_size\n",
    "            #)\n",
    "\n",
    "            #transitions = emitter_state.replay_buffer.sample(emitter_state.buffer_state, key).experience\n",
    "            (\n",
    "                new_policy_params,\n",
    "                new_policy_opt_states,\n",
    "            ) = batched_policy_update_fn(policy_params, policy_opt_state)#transitions)\n",
    "            return (new_policy_params, new_policy_opt_states, key), ()\n",
    "\n",
    "        (\n",
    "            final_policy_params,\n",
    "            final_policy_opt_state,\n",
    "            final_key,\n",
    "        ), _ = jax.lax.scan(\n",
    "            scan_update_policies,\n",
    "            (parents, policy_opt_states, emitter_state.key),\n",
    "            length=self._config.num_pg_training_steps,\n",
    "        )\n",
    "\n",
    "        return final_policy_params\n",
    "\n",
    "    def emit_actor(self, emitter_state: CustomQualityPGEmitterState) -> Genotype:\n",
    "        \"\"\"Emit the greedy actor.\n",
    "\n",
    "        Simply needs to be retrieved from the emitter state.\n",
    "\n",
    "        Args:\n",
    "            emitter_state: the current emitter state, it stores the\n",
    "                greedy actor.\n",
    "\n",
    "        Returns:\n",
    "            The parameters of the actor.\n",
    "        \"\"\"\n",
    "        return emitter_state.actor_params\n",
    "\n",
    "    def state_update(  # type: ignore\n",
    "        self,\n",
    "        emitter_state: CustomQualityPGEmitterState,\n",
    "        repertoire: Optional[Repertoire],\n",
    "        genotypes: Optional[Genotype],\n",
    "        fitnesses: Optional[Fitness],\n",
    "        descriptors: Optional[Descriptor],\n",
    "        extra_scores: ExtraScores,\n",
    "    ) -> CustomQualityPGEmitterState:\n",
    "        \"\"\"This function gives an opportunity to update the emitter state\n",
    "        after the genotypes have been scored.\n",
    "\n",
    "        Here it is used to fill the Replay Buffer with the transitions\n",
    "        from the scoring of the genotypes, and then the training of the\n",
    "        critic/actor happens. Hence the params of critic/actor are updated,\n",
    "        as well as their optimizer states.\n",
    "\n",
    "        Args:\n",
    "            emitter_state: current emitter state.\n",
    "            repertoire: the current genotypes repertoire\n",
    "            genotypes: unused here - but compulsory in the signature.\n",
    "            fitnesses: unused here - but compulsory in the signature.\n",
    "            descriptors: unused here - but compulsory in the signature.\n",
    "            extra_scores: extra information coming from the scoring function,\n",
    "                this contains the transitions added to the replay buffer.\n",
    "\n",
    "        Returns:\n",
    "            New emitter state where the replay buffer has been filled with\n",
    "            the new experienced transitions.\n",
    "        \"\"\"\n",
    "        # get the transitions out of the dictionary\n",
    "        assert \"transitions\" in extra_scores.keys(), \"Missing transitions or wrong key\"\n",
    "        transitions = extra_scores[\"transitions\"]\n",
    "\n",
    "        def prepare_for_buffer(x):\n",
    "            # x: (P, T, E, *feat_dims)\n",
    "            P, T, E, *feat_dims = x.shape\n",
    "\n",
    "            # First move to (P, E, T, *feat_dims)\n",
    "            x = x.transpose(0, 2, 1, *range(3, x.ndim))\n",
    "\n",
    "            # Merge P * E into batch\n",
    "            x = x.reshape(P * E, T, *feat_dims)\n",
    "\n",
    "            # Add dummy seq_len=1 in the middle -> (batch, 1, T, feat_dim...)\n",
    "            x = x[:, np.newaxis, ...]\n",
    "\n",
    "            return x\n",
    "        \n",
    "        buffer_traj_batch = jax.tree.map(\n",
    "            prepare_for_buffer,\n",
    "            transitions\n",
    "        )\n",
    "        #buffer_traj_batch = jax.tree.map(\n",
    "        #    lambda x: jnp.swapaxes(x, 0, 1)[\n",
    "        #        :, np.newaxis\n",
    "        #    ],  # put the batch dim first and add a dummy sequence dim\n",
    "        #    transitions,\n",
    "        #)  # (num_envs, 1, time_steps, ...)\n",
    "        #buffer_traj_batch = jax.tree.map(\n",
    "        #    lambda x: jnp.swapaxes(\n",
    "        #        # Reorder from (num_policies, time_steps, num_envs, …)\n",
    "        #        # → (time_steps, num_policies, num_envs, …)\n",
    "        #        x.transpose(1, 0, 2, *range(3, x.ndim))\n",
    "        #        # Merge num_policies * num_envs into one dimension\n",
    "        #        .reshape(x.shape[1], -1, *x.shape[3:]),\n",
    "        #        0, 1  # swap (time_steps, merged_envs, …) → (merged_envs, time_steps, …)\n",
    "        #    )[:, np.newaxis],  # add dummy sequence dim\n",
    "        #    transitions,\n",
    "        #)\n",
    "        new_buffer_state = self._buffer.add(emitter_state.buffer_state, buffer_traj_batch)\n",
    "        final_emitter_state = emitter_state.replace(buffer_state=new_buffer_state)\n",
    "\n",
    "        # add transitions in the replay buffer\n",
    "        #emitter_state = emitter_state.replace(\n",
    "        #    replay_buffer=emitter_state.replay_buffer.insert(transitions)\n",
    "        #)\n",
    "        \n",
    "        # Conduct Actor-Critic training\n",
    "        #final_emitter_state, _ = jax.lax.scan(\n",
    "        #    self._scan_actor_critic_training,\n",
    "        #    emitter_state,\n",
    "        #    length=self._actor_critic_iterations,\n",
    "        #)\n",
    "\n",
    "        return final_emitter_state  # type: ignore\n",
    "\n",
    "    def _update_policy(\n",
    "        self,\n",
    "        policy_params: Params,\n",
    "        policy_opt_state: optax.OptState,\n",
    "        #transitions: QDTransition,\n",
    "        emitter_state: CustomQualityPGEmitterState,\n",
    "        #critic_params: Params,\n",
    "    ) -> Tuple[optax.OptState, Params]:\n",
    "        \"\"\"\n",
    "        Perform one step of PG update on the off-spring policy.\n",
    "        This function is vmapped to mutate the entire batch of off-springs\n",
    "        in parallel.\n",
    "\n",
    "        Args:\n",
    "            policy_params: the parameters of the policy.\n",
    "            policy_opt_state: the optimiser state of the policy.\n",
    "            transitions: a mini-batch of transitions for gradient computation\n",
    "            critic_params: the parameters of the critic networks serving as\n",
    "                a differentiable target. This is fixed in each iteration.\n",
    "\n",
    "        Returns:\n",
    "            new_policy_params: new policy parameters\n",
    "            new_policy_opt_state: updated optimiser state\n",
    "        \"\"\"\n",
    "        new_target_params = policy_params\n",
    "\n",
    "        # SAMPLE minibatches for each grad update\n",
    "        key = emitter_state.key\n",
    "        minibatches_list = []\n",
    "\n",
    "        for _ in range(self._config.target_update_interval):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            minibatch = self._buffer.sample(emitter_state.buffer_state, subkey).experience\n",
    "            minibatches_list.append(minibatch)\n",
    "\n",
    "        # Stack into [num_updates, ...] pytree\n",
    "        minibatches = jax.tree.map(lambda *xs: jnp.stack(xs), *minibatches_list)\n",
    "\n",
    "        # Update key back into emitter_state\n",
    "        emitter_state = emitter_state.replace(key=key)\n",
    "\n",
    "        def _apply_grad_update(carry, minibatch):\n",
    "            # Compute the policy gradient\n",
    "            policy_params, policy_opt_state = carry\n",
    "\n",
    "            policy_gradient = jax.grad(self._policy_loss_fn)(\n",
    "                policy_params,\n",
    "                #critic_params,\n",
    "                #transitions,\n",
    "                new_target_params,\n",
    "                #emitter_state\n",
    "                minibatch\n",
    "            )\n",
    "            # Apply the update on the policy\n",
    "            (\n",
    "                policy_updates,\n",
    "                new_policy_opt_state,\n",
    "            ) = self._policies_optimizer.update(policy_gradient, policy_opt_state)\n",
    "            new_policy_params = optax.apply_updates(policy_params, policy_updates)\n",
    "\n",
    "            return (new_policy_params, new_policy_opt_state), ()\n",
    "        \n",
    "        (new_policy_params, new_policy_opt_state), _ = jax.lax.scan(\n",
    "            _apply_grad_update, \n",
    "            (policy_params, policy_opt_state), \n",
    "            #length=config[\"TARGET_UPDATE_INTERVAL\"]\n",
    "            minibatches,\n",
    "            )\n",
    "        \n",
    "        new_target_params = optax.incremental_update(\n",
    "                        new_policy_params, #train_state.params,\n",
    "                        new_target_params, #train_state.target_network_params,\n",
    "                        self._config.tau,\n",
    "                    )\n",
    "\n",
    "        return new_target_params, new_policy_opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a83d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myPGAMEEmitter(MultiEmitter):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: PGAMEConfig,\n",
    "        policy_network: nn.Module,\n",
    "        env: QDEnv,\n",
    "        variation_fn: Callable[[Params, Params, RNGKey], Tuple[Params, RNGKey]],\n",
    "        selector: Optional[Selector] = None,\n",
    "    ) -> None:\n",
    "\n",
    "        self._config = config\n",
    "        self._policy_network = policy_network\n",
    "        self._env = env\n",
    "        self._variation_fn = variation_fn\n",
    "\n",
    "        ga_batch_size = int(self._config.proportion_mutation_ga * config.env_batch_size)\n",
    "        qpg_batch_size = config.env_batch_size - ga_batch_size\n",
    "\n",
    "        @dataclass\n",
    "        class CustomQualityPGConfig(QualityPGConfig):\n",
    "            num_envs: Any = None\n",
    "            num_steps: Any = None\n",
    "            target_update_interval: Any = None\n",
    "            tau: Any = None\n",
    "                \n",
    "        qpg_config = CustomQualityPGConfig(\n",
    "            env_batch_size=qpg_batch_size,\n",
    "            num_critic_training_steps=config.num_critic_training_steps,\n",
    "            num_pg_training_steps=config.num_pg_training_steps,\n",
    "            replay_buffer_size=config.replay_buffer_size,\n",
    "            critic_hidden_layer_size=config.critic_hidden_layer_size,\n",
    "            critic_learning_rate=config.critic_learning_rate,\n",
    "            actor_learning_rate=config.greedy_learning_rate,\n",
    "            policy_learning_rate=config.policy_learning_rate,\n",
    "            noise_clip=config.noise_clip,\n",
    "            policy_noise=config.policy_noise,\n",
    "            discount=config.discount,\n",
    "            reward_scaling=config.reward_scaling,\n",
    "            batch_size=config.batch_size,\n",
    "            soft_tau_update=config.soft_tau_update,\n",
    "            policy_delay=config.policy_delay,\n",
    "\n",
    "            num_envs = config.num_envs,\n",
    "            num_steps = config.num_steps,\n",
    "            target_update_interval = config.target_update_interval,\n",
    "            tau = config.tau,\n",
    "        )\n",
    "\n",
    "        # define the quality emitter\n",
    "        q_emitter = myQualityPGEmitter(\n",
    "            config=qpg_config,\n",
    "            policy_network=policy_network,\n",
    "            env=env,\n",
    "            selector=selector,\n",
    "        )\n",
    "\n",
    "        # define the GA emitter\n",
    "        ga_emitter = MixingEmitter(\n",
    "            mutation_fn=lambda x, r: (x, r),\n",
    "            variation_fn=variation_fn,\n",
    "            variation_percentage=1.0,\n",
    "            batch_size=ga_batch_size,\n",
    "            selector=selector,\n",
    "        )\n",
    "\n",
    "        super().__init__(emitters=(q_emitter, ga_emitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aa40cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_mutation_ga = 0.5 \n",
    "\n",
    "# IQL/TD3 params\n",
    "env_batch_size = batch_size #100 \n",
    "replay_buffer_size = config[\"BUFFER_SIZE\"] #1000000 \n",
    "critic_hidden_layer_size = (256, 256) \n",
    "critic_learning_rate = 3e-4 \n",
    "greedy_learning_rate = 3e-4 \n",
    "policy_learning_rate = 1e-3 \n",
    "noise_clip = 0.5\n",
    "policy_noise = 0.2 \n",
    "discount = config[\"GAMMA\"] #0.99 \n",
    "reward_scaling = 1.0 \n",
    "transitions_batch_size = config[\"BUFFER_BATCH_SIZE\"] #256 \n",
    "soft_tau_update = 0.005\n",
    "num_critic_training_steps = 300 \n",
    "num_pg_training_steps = 100 \n",
    "policy_delay = 2\n",
    "\n",
    "# Define the PG-emitter config\n",
    "@dataclass\n",
    "class CustomPGAMEConfig(PGAMEConfig):\n",
    "    num_envs: Any = None\n",
    "    num_steps: Any = None\n",
    "    target_update_interval: Any = None\n",
    "    tau: Any = None\n",
    "\n",
    "pga_emitter_config = CustomPGAMEConfig(\n",
    "    env_batch_size=env_batch_size,\n",
    "    batch_size=transitions_batch_size,\n",
    "    proportion_mutation_ga=proportion_mutation_ga,\n",
    "    critic_hidden_layer_size=critic_hidden_layer_size,\n",
    "    critic_learning_rate=critic_learning_rate,\n",
    "    greedy_learning_rate=greedy_learning_rate,\n",
    "    policy_learning_rate=policy_learning_rate,\n",
    "    noise_clip=noise_clip,\n",
    "    policy_noise=policy_noise,\n",
    "    discount=discount,\n",
    "    reward_scaling=reward_scaling,\n",
    "    replay_buffer_size=replay_buffer_size,\n",
    "    soft_tau_update=soft_tau_update,\n",
    "    num_critic_training_steps=num_critic_training_steps,\n",
    "    num_pg_training_steps=num_pg_training_steps,\n",
    "    policy_delay=policy_delay,\n",
    "\n",
    "    num_envs = config[\"NUM_ENVS\"],\n",
    "    num_steps = config[\"NUM_STEPS\"],\n",
    "    target_update_interval = config[\"TARGET_UPDATE_INTERVAL\"],\n",
    "    tau = config[\"TAU\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf096879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the scoring function\n",
    "# descriptor_extraction_fn = environments.descriptor_extractor[env_name] # Need to write my own function to extract descriptors\n",
    "scoring_fn = functools.partial(\n",
    "    scoring_function,\n",
    "    episode_length=episode_length,\n",
    "    play_reset_fn=reset_fn,\n",
    "    play_step_fn=play_step_fn,\n",
    "    descriptor_extractor=descriptor_extraction_fn,\n",
    ")\n",
    "\n",
    "# Get minimum reward value to make sure qd_score are positive\n",
    "#reward_offset = environments.reward_offset[env_name]\n",
    "\n",
    "# Define a metrics function\n",
    "metrics_function = functools.partial(\n",
    "    default_qd_metrics,\n",
    "    qd_offset=1#reward_offset * episode_length, # Used to ensure QD score is positive could set to 1 if not needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94bdbaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emitter\n",
    "variation_fn = functools.partial(\n",
    "    isoline_variation, iso_sigma=iso_sigma, line_sigma=line_sigma\n",
    ")\n",
    "\n",
    "mixing_emitter = MixingEmitter(\n",
    "    mutation_fn=None,\n",
    "    variation_fn=variation_fn,\n",
    "    variation_percentage=1.0,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "pg_emitter = myPGAMEEmitter(\n",
    "    config=pga_emitter_config,\n",
    "    policy_network=policy_network,\n",
    "    env=wrapped_env, #env,\n",
    "    variation_fn=variation_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7dfea00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids: (2000, 2)\n",
      "Descriptor dim: 2\n",
      "Network params size (MB): 29.98528\n",
      "Expected repertoire memory (MB): 59970.56\n",
      "hstate shape: (5, 1, 128)\n",
      "_obs shape: (5, 1, 1, 132)\n",
      "_dones shape: (5, 1, 1)\n",
      "new_hstate shape: (5, 1, 128)\n",
      "actions {'ally_0': Traced<ShapedArray(int32[1])>with<DynamicJaxprTrace>, 'ally_1': Traced<ShapedArray(int32[1])>with<DynamicJaxprTrace>, 'ally_2': Traced<ShapedArray(int32[1])>with<DynamicJaxprTrace>, 'ally_3': Traced<ShapedArray(int32[1])>with<DynamicJaxprTrace>, 'ally_4': Traced<ShapedArray(int32[1])>with<DynamicJaxprTrace>}\n",
      "data.reward shape: (64, 128, 5)\n",
      "data.global_done shape: (64, 128, 5)\n",
      "mask shape: (64, 128, 5)\n",
      "fitness shape: (64, 1)\n",
      "descriptors shape: (64, 2)\n",
      "Init sample_traj shape: Timestep(obs={'__all__': (128, 120), 'ally_0': (128, 132), 'ally_1': (128, 132), 'ally_2': (128, 132), 'ally_3': (128, 132), 'ally_4': (128, 132)}, actions={'ally_0': (128,), 'ally_1': (128,), 'ally_2': (128,), 'ally_3': (128,), 'ally_4': (128,)}, rewards={'__all__': (128,), 'ally_0': (128,), 'ally_1': (128,), 'ally_2': (128,), 'ally_3': (128,), 'ally_4': (128,)}, dones={'__all__': (128,), 'ally_0': (128,), 'ally_1': (128,), 'ally_2': (128,), 'ally_3': (128,), 'ally_4': (128,)}, avail_actions={'ally_0': (128, 10), 'ally_1': (128, 10), 'ally_2': (128, 10), 'ally_3': (128, 10), 'ally_4': (128, 10)})\n",
      "Runtime sample_traj shape: Timestep(obs={'__all__': (64, 1, 128, 120), 'ally_0': (64, 1, 128, 132), 'ally_1': (64, 1, 128, 132), 'ally_2': (64, 1, 128, 132), 'ally_3': (64, 1, 128, 132), 'ally_4': (64, 1, 128, 132)}, actions={'ally_0': (64, 1, 128), 'ally_1': (64, 1, 128), 'ally_2': (64, 1, 128), 'ally_3': (64, 1, 128), 'ally_4': (64, 1, 128)}, rewards={'__all__': (64, 1, 128), 'ally_0': (64, 1, 128), 'ally_1': (64, 1, 128), 'ally_2': (64, 1, 128), 'ally_3': (64, 1, 128), 'ally_4': (64, 1, 128)}, dones={'__all__': (64, 1, 128), 'ally_0': (64, 1, 128), 'ally_1': (64, 1, 128), 'ally_2': (64, 1, 128), 'ally_3': (64, 1, 128), 'ally_4': (64, 1, 128)}, avail_actions={'ally_0': (64, 1, 128, 10), 'ally_1': (64, 1, 128, 10), 'ally_2': (64, 1, 128, 10), 'ally_3': (64, 1, 128, 10), 'ally_4': (64, 1, 128, 10)})\n"
     ]
    }
   ],
   "source": [
    "# Instantiate MAP-Elites\n",
    "map_elites = MAPElites(\n",
    "    scoring_function=scoring_fn,\n",
    "    emitter=pg_emitter, #mixing_emitter,\n",
    "    metrics_function=metrics_function,\n",
    ")\n",
    "\n",
    "# Compute the centroids\n",
    "key, subkey = jax.random.split(key)\n",
    "centroids = compute_cvt_centroids(\n",
    "    num_descriptors=number_of_descriptors,#env.descriptor_length, # Num of dimensions in the descriptor\n",
    "    num_init_cvt_samples=num_init_cvt_samples,\n",
    "    num_centroids=num_centroids,\n",
    "    minval=min_descriptor,\n",
    "    maxval=max_descriptor,\n",
    "    key=subkey,\n",
    ")\n",
    "\n",
    "print(\"Centroids:\", centroids.shape)   # how many?\n",
    "print(\"Descriptor dim:\", centroids.shape[-1])\n",
    "\n",
    "param_bytes = sum(x.size * x.dtype.itemsize for x in jax.tree.leaves(network_params))\n",
    "print(\"Network params size (MB):\", param_bytes / 1e6)\n",
    "\n",
    "total_repertoire_mem_mb = param_bytes/1e6 * centroids.shape[0]\n",
    "print(\"Expected repertoire memory (MB):\", total_repertoire_mem_mb)\n",
    "\n",
    "# Compute initial repertoire and emitter state\n",
    "key, subkey = jax.random.split(key)\n",
    "repertoire, emitter_state, init_metrics = map_elites.init(network_params, centroids, subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d251ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTreeDef((CustomNode(MapElitesRepertoire[((),)], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}, 'ScannedRNN_0': {'GRUCell_0': {'hn': {'bias': *, 'kernel': *}, 'hr': {'kernel': *}, 'hz': {'kernel': *}, 'in': {'bias': *, 'kernel': *}, 'ir': {'bias': *, 'kernel': *}, 'iz': {'bias': *, 'kernel': *}}}}}, *, {}, *, *]), CustomNode(MultiEmitterState[()], [(CustomNode(CustomQualityPGEmitterState[('buffer_state', 'key')], [CustomNode(TrajectoryBufferState[('current_index', 'experience', 'is_full')], [*, CustomNode(Timestep[('actions', 'avail_actions', 'dones', 'obs', 'rewards')], [{'ally_0': *, 'ally_1': *, 'ally_2': *, 'ally_3': *, 'ally_4': *}, {'ally_0': *, 'ally_1': *, 'ally_2': *, 'ally_3': *, 'ally_4': *}, {'__all__': *, 'ally_0': *, 'ally_1': *, 'ally_2': *, 'ally_3': *, 'ally_4': *}, {'__all__': *, 'ally_0': *, 'ally_1': *, 'ally_2': *, 'ally_3': *, 'ally_4': *}, {'__all__': *, 'ally_0': *, 'ally_1': *, 'ally_2': *, 'ally_3': *, 'ally_4': *}]), *]), *]), None)]), *))\n",
      "hstate shape: (5, 1, 128)\n",
      "_obs shape: (5, 1, 1, 132)\n",
      "_dones shape: (5, 1, 1)\n",
      "new_hstate shape: (5, 1, 128)\n",
      "actions {'ally_0': Traced<ShapedArray(int32[1])>with<DynamicJaxprTrace>, 'ally_1': Traced<ShapedArray(int32[1])>with<DynamicJaxprTrace>, 'ally_2': Traced<ShapedArray(int32[1])>with<DynamicJaxprTrace>, 'ally_3': Traced<ShapedArray(int32[1])>with<DynamicJaxprTrace>, 'ally_4': Traced<ShapedArray(int32[1])>with<DynamicJaxprTrace>}\n",
      "data.reward shape: (64, 128, 5)\n",
      "data.global_done shape: (64, 128, 5)\n",
      "mask shape: (64, 128, 5)\n",
      "fitness shape: (64, 1)\n",
      "descriptors shape: (64, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-22 15:56:28.834895: W external/xla/xla/hlo/transforms/simplifiers/hlo_rematerialization.cc:3021] Can't reduce memory use below 7.87GiB (8455146708 bytes) by rematerialization; only reduced to 10.19GiB (10942518209 bytes), down from 10.23GiB (10984469573 bytes) originally\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num loop: 122\n"
     ]
    }
   ],
   "source": [
    "log_period = 10\n",
    "num_loops = num_iterations // log_period\n",
    "\n",
    "# Initialize metrics\n",
    "metrics = {key: jnp.array([]) for key in [\"iteration\", \"qd_score\", \"coverage\", \"max_fitness\", \"time\"]}\n",
    "\n",
    "# Set up init metrics\n",
    "init_metrics = jax.tree.map(lambda x: jnp.array([x]) if x.shape == () else x, init_metrics)\n",
    "init_metrics[\"iteration\"] = jnp.array([0], dtype=jnp.int32)\n",
    "init_metrics[\"time\"] = jnp.array([0.0])  # No time recorded for initialization\n",
    "\n",
    "# Convert init_metrics to match the metrics dictionary structure\n",
    "metrics = jax.tree.map(lambda metric, init_metric: jnp.concatenate([metric, init_metric], axis=0), metrics, init_metrics)\n",
    "\n",
    "# Initialize CSV logger\n",
    "#csv_logger = CSVLogger(\n",
    "#    \"mapelites-logs.csv\",\n",
    "#    header=list(metrics.keys())\n",
    "#)\n",
    "\n",
    "# Log initial metrics\n",
    "#csv_logger.log(jax.tree.map(lambda x: x[-1], metrics))\n",
    "\n",
    "# Main loop\n",
    "map_elites_scan_update = map_elites.scan_update\n",
    "print(jax.tree_util.tree_structure((repertoire, emitter_state, key)))\n",
    "\n",
    "for i in range(num_loops):\n",
    "    start_time = time.time()\n",
    "    (\n",
    "        repertoire,\n",
    "        emitter_state,\n",
    "        key,\n",
    "    ), current_metrics = jax.lax.scan(\n",
    "        map_elites_scan_update,\n",
    "        (repertoire, emitter_state, key),\n",
    "        (),\n",
    "        length=log_period,\n",
    "    )\n",
    "    timelapse = time.time() - start_time\n",
    "    print(\"num loop:\", num_loops)\n",
    "\n",
    "    # Metrics\n",
    "    current_metrics[\"iteration\"] = jnp.arange(1+log_period*i, 1+log_period*(i+1), dtype=jnp.int32)\n",
    "    current_metrics[\"time\"] = jnp.repeat(timelapse, log_period)\n",
    "    metrics = jax.tree.map(lambda metric, current_metric: jnp.concatenate([metric, current_metric], axis=0), metrics, current_metrics)\n",
    "\n",
    "    # Log\n",
    "    #csv_logger.log(jax.tree.map(lambda x: x[-1], metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217deb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the x-axis array\n",
    "#env_steps = metrics[\"iteration\"]\n",
    "env_steps = num_iterations * batch_size * episode_length * config[\"NUM_ENVS\"]\n",
    "#print(jnp.max(metrics[\"iteration\"]))\n",
    "\n",
    "%matplotlib inline\n",
    "# Create the plots and the grid\n",
    "fig, axes = plot_map_elites_results(env_steps=env_steps, metrics=metrics, repertoire=repertoire, min_descriptor=min_descriptor, max_descriptor=max_descriptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc2cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = jnp.argmax(repertoire.fitnesses)\n",
    "best_fitness = jnp.max(repertoire.fitnesses)\n",
    "best_descriptor = repertoire.descriptors[best_idx]\n",
    "print(\n",
    "    f\"Best fitness in the repertoire: {best_fitness:.2f}\\n\",\n",
    "    f\"Descriptor of the best individual in the repertoire: {best_descriptor}\\n\",\n",
    "    f\"Index in the repertoire of this individual: {best_idx}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317021d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the parameters of the best individual\n",
    "my_params = jax.tree.map(\n",
    "    lambda x: x[best_idx],\n",
    "    repertoire.genotypes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_obs_with_id(obs_dict, env):\n",
    "    \"\"\"Simulate CTRolloutManager's preprocessing by adding one-hot agent IDs.\"\"\"\n",
    "    new_obs_dict = {}\n",
    "    num_agents = len(env.agents)\n",
    "    for i, agent in enumerate(env.agents):\n",
    "        obs = obs_dict[agent].flatten()\n",
    "        one_hot = jax.nn.one_hot(i, num_classes=num_agents)\n",
    "        new_obs_dict[agent] = jnp.concatenate([obs, one_hot])\n",
    "    return new_obs_dict\n",
    "\n",
    "\n",
    "def visualize_recurrent_policy(trained_params, env, config):\n",
    "    rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "    rng, reset_rng = jax.random.split(rng)\n",
    "    #wrapped_env = CTRolloutManager(env, batch_size=1)\n",
    "\n",
    "    # Create policy network\n",
    "    #network = RNNQNetwork(\n",
    "    #    action_dim=wrapped_env.max_action_space,\n",
    "    #    hidden_dim=config[\"HIDDEN_SIZE\"],\n",
    "    #)\n",
    "    network = RNNQNetwork(\n",
    "        action_dim=env.action_space(env.agents[0]).n,\n",
    "        hidden_dim=config[\"HIDDEN_SIZE\"],\n",
    "    )\n",
    "    \n",
    "    # Reset environment\n",
    "    #obs, env_state = wrapped_env.batch_reset(reset_rng)\n",
    "    obs, env_state = env.reset(reset_rng)\n",
    "    #dones = {\n",
    "    #    agent: jnp.zeros((1), dtype=bool)\n",
    "    #    for agent in env.agents + [\"__all__\"]\n",
    "    #}\n",
    "    dones = {agent: jnp.array(False) for agent in env.agents}\n",
    "    hstate = ScannedRNN.initialize_carry(\n",
    "        config[\"HIDDEN_SIZE\"], len(env.agents), 1\n",
    "    )\n",
    "    \n",
    "    # Collect all environment states\n",
    "    returns = {agent: 0.0 for agent in env.agents}\n",
    "    state_seq = []\n",
    "    max_steps = config[\"NUM_STEPS\"]\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Compute Q-values\n",
    "        # Prepare inputs for network\n",
    "        obs = preprocess_obs_with_id(obs, env)\n",
    "        _obs = batchify(obs)         # (num_agents, obs_dim)\n",
    "        _obs = _obs[:, None, :]                      # (num_agents, 1, obs_dim)\n",
    "\n",
    "        #_dones = batchify(dones)    # (num_agents,)\n",
    "        #_dones = _dones[:, None]                     # (num_agents, 1)\n",
    "        _dones = jnp.stack([jnp.array([dones[agent]]) for agent in env.agents])  # shape (num_agents, 1)\n",
    "        _dones = jnp.expand_dims(_dones, axis=-1)  # from (3, 1) to (3, 1, 1)\n",
    "\n",
    "        #print(\"_obs.shape:\", _obs.shape)\n",
    "        #print(\"_dones.shape:\", _dones.shape)\n",
    "        #print(\"hstate.shape:\", hstate.shape)\n",
    "\n",
    "        def apply_fn(h, o, d):\n",
    "            return network.apply(trained_params, h, o, d)\n",
    "\n",
    "        hstate, q_vals = jax.vmap(apply_fn, in_axes=(0, 0, 0))(\n",
    "            hstate,\n",
    "            _obs,\n",
    "            _dones,\n",
    "        )\n",
    "        #hstate = hstate[:, None, :]  # Already in (num_agents, hidden_dim)\n",
    "        q_vals = q_vals.squeeze(axis=1)  # (num_agents, num_envs, num_actions) remove the time dim\n",
    "        #print(\"q_vals.shape\", q_vals.shape)\n",
    "        \n",
    "        actions = {}\n",
    "        #avail_actions = wrapped_env.get_valid_actions(env_state.env_state)\n",
    "        avail_actions = env.get_avail_actions(env_state.env_state)\n",
    "\n",
    "        for i, agent in enumerate(env.agents):\n",
    "            avail_agent = avail_actions[agent][None, None, :]  # shape (1, 1, n_actions)\n",
    "            #print(\"avail_agent.shape\", avail_agent.shape)\n",
    "            \n",
    "            unavail_actions = 1 - avail_agent  # shape (1, 1, n_actions)\n",
    "            \n",
    "            # Select Q-values for this agent only\n",
    "            q_agent = q_vals[i][None, None, :]  # shape (1, 1, n_actions)\n",
    "            q_masked = q_agent - (unavail_actions * 1e10)\n",
    "\n",
    "            action = jnp.argmax(q_masked, axis=-1)  # shape (1, 1)\n",
    "            action = action.squeeze()               # scalar\n",
    "            #print(\"action.shape\", action.shape)\n",
    "\n",
    "            # Wrap in array with batch dim\n",
    "            actions[agent] = int(action)    # shape (1,)\n",
    "        \n",
    "        rng, rng_s = jax.random.split(rng)\n",
    "        state_seq.append((rng_s, env_state.env_state, actions))\n",
    "\n",
    "        # Step environment\n",
    "\n",
    "        # Batch the actions dict\n",
    "        # Original actions: {'ally_0': 4, 'ally_1': 4, 'ally_2': 4}\n",
    "        #actions = {k: jnp.array([v]) for k, v in actions.items()}\n",
    "\n",
    "        #obs, env_state, rewards, dones, infos = wrapped_env.batch_step(\n",
    "        #    rng_s, env_state, actions\n",
    "        #)\n",
    "        obs, env_state, rewards, dones, infos = env.step(rng_s, env_state, actions)\n",
    "        returns = {a: returns[a] + rewards[a] for a in env.agents}\n",
    "        \n",
    "        if dones[\"__all__\"]:\n",
    "            break\n",
    "\n",
    "    # Visualization\n",
    "    print(\"Returns:\", returns)\n",
    "\n",
    "    viz = SMAXVisualizer(env, state_seq)\n",
    "    viz.animate(view=False, save_fname=\"trained_iql_rnn.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = map_name_to_scenario(config[\"MAP_NAME\"])\n",
    "env = HeuristicEnemySMAX(scenario=scenario, **config[\"ENV_KWARGS\"])\n",
    "env = SMAXLogWrapper(env)\n",
    "\n",
    "visualize_recurrent_policy(my_params, env, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JaxMARL (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
