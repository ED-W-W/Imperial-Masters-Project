{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ab03635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from typing import Sequence, NamedTuple, Any, Dict\n",
    "from flax.training.train_state import TrainState\n",
    "import distrax\n",
    "\n",
    "from jaxmarl import make\n",
    "from jaxmarl.wrappers.baselines import (\n",
    "    SMAXLogWrapper,\n",
    "    MPELogWrapper,\n",
    "    LogWrapper,\n",
    "    CTRolloutManager,\n",
    ")\n",
    "from jaxmarl.environments.smax import map_name_to_scenario, HeuristicEnemySMAX\n",
    "from jaxmarl.viz.visualizer import Visualizer, SMAXVisualizer\n",
    "\n",
    "import functools\n",
    "from functools import partial\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from qdax.core.map_elites import MAPElites\n",
    "from qdax.core.containers.mapelites_repertoire import compute_cvt_centroids, MapElitesRepertoire\n",
    "#import qdax.tasks.brax.v1 as environments\n",
    "#from qdax.tasks.brax.v1.env_creators import scoring_function_brax_envs as scoring_function\n",
    "from qdax.core.neuroevolution.buffers.buffer import QDTransition\n",
    "from qdax.core.neuroevolution.networks.networks import MLP\n",
    "from qdax.core.emitters.mutation_operators import isoline_variation\n",
    "from qdax.core.emitters.standard_emitters import MixingEmitter\n",
    "from qdax.utils.plotting import plot_map_elites_results\n",
    "from qdax.utils.metrics import CSVLogger, default_qd_metrics\n",
    "\n",
    "## Brax version conflict with JaxMARL(0.10.3) and QDax(0.10.4 / 0.12.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13706c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional, Tuple\n",
    "from qdax.custom_types import (\n",
    "    Descriptor,\n",
    "    EnvState,\n",
    "    ExtraScores,\n",
    "    Fitness,\n",
    "    Genotype,\n",
    "    Observation,\n",
    "    Params,\n",
    "    RNGKey,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "307a039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64# 128 # Num of offsprings \n",
    "#env_name = 'walker2d_uni'\n",
    "episode_length = 128  # NUM_STEPS\n",
    "num_iterations = 610  # TOTAL_TIMESTEPS / (NUM_ENVS × NUM_STEPS)\n",
    "seed = 88 \n",
    "policy_hidden_layer_sizes = (128, 128, 128) #(64, 64)\n",
    "iso_sigma = 0.05 #0.005 \n",
    "line_sigma = 0.5 #0.05 \n",
    "num_init_cvt_samples = 20000 #50000 \n",
    "num_centroids = 2000 #1024 \n",
    "min_descriptor = 0. \n",
    "max_descriptor = 1.0 \n",
    "number_of_descriptors=2\n",
    "\n",
    "config = {\n",
    "    # valid for iql, vdn, qmix\n",
    "    \"TOTAL_TIMESTEPS\": 1e7,\n",
    "    \"NUM_ENVS\": 128, #16,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"BUFFER_SIZE\": 5000,\n",
    "    \"BUFFER_BATCH_SIZE\": 32,\n",
    "    \"HIDDEN_SIZE\": 512,\n",
    "    \"MIXER_EMBEDDING_DIM\": 64,\n",
    "    \"MIXER_HYPERNET_HIDDEN_DIM\": 256,\n",
    "    \"MIXER_INIT_SCALE\": 0.001,\n",
    "    \"EPS_START\": 1.0,\n",
    "    \"EPS_FINISH\": 0.05,\n",
    "    \"EPS_DECAY\": 0.1, # percentage of updates\n",
    "    \"MAX_GRAD_NORM\": 10,\n",
    "    \"TARGET_UPDATE_INTERVAL\": 10,\n",
    "    \"TAU\": 1.,\n",
    "    \"NUM_EPOCHS\": 8,\n",
    "    \"LR\": 0.00005,\n",
    "    \"LEARNING_STARTS\": 10000, # timesteps\n",
    "    \"LR_LINEAR_DECAY\": False,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"REW_SCALE\": 10., # scale the reward to the original scale of SMAC\n",
    "\n",
    "    # ENV\n",
    "    \"ENV_NAME\": \"HeuristicEnemySMAX\",\n",
    "    \"MAP_NAME\": \"3s_vs_5z\",\n",
    "    #\"MAP_NAME\": \"2s3z\",\n",
    "    \"ENV_KWARGS\": {\n",
    "        \"see_enemy_actions\": True,\n",
    "        \"walls_cause_death\": True,\n",
    "        \"attack_mode\": \"closest\",\n",
    "    },\n",
    "\n",
    "    \"NUM_SEEDS\": 1, # number of vmapped seeds\n",
    "    \"SEED\": 0,\n",
    "\n",
    "    \"HYP_TUNE\": False, # perform hyp tune\n",
    "\n",
    "    # evaluate\n",
    "    \"TEST_DURING_TRAINING\": False, #True,\n",
    "    \"TEST_INTERVAL\": 0.05, # as a fraction of updates, i.e. log every 5% of training process\n",
    "    \"TEST_NUM_STEPS\": 128,\n",
    "    \"TEST_NUM_ENVS\": 512, # number of episodes to average over, can affect performance\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4647e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScannedRNN(nn.Module):\n",
    "\n",
    "    @partial(\n",
    "        nn.scan,\n",
    "        variable_broadcast=\"params\",\n",
    "        in_axes=0,\n",
    "        out_axes=0,\n",
    "        split_rngs={\"params\": False},\n",
    "    )\n",
    "    @nn.compact\n",
    "    def __call__(self, carry, x):\n",
    "        \"\"\"Applies the module.\"\"\"\n",
    "        rnn_state = carry\n",
    "        ins, resets = x\n",
    "        hidden_size = ins.shape[-1]\n",
    "        rnn_state = jnp.where(\n",
    "            resets[:, np.newaxis],\n",
    "            self.initialize_carry(hidden_size, *ins.shape[:-1]),\n",
    "            rnn_state,\n",
    "        )\n",
    "        new_rnn_state, y = nn.GRUCell(hidden_size)(rnn_state, ins)\n",
    "        return new_rnn_state, y\n",
    "\n",
    "    @staticmethod\n",
    "    def initialize_carry(hidden_size, *batch_size):\n",
    "        # Use a dummy key since the default state init fn is just zeros.\n",
    "        return nn.GRUCell(hidden_size, parent=None).initialize_carry(\n",
    "            jax.random.PRNGKey(0), (*batch_size, hidden_size)\n",
    "        )\n",
    "\n",
    "\n",
    "class RNNQNetwork(nn.Module):\n",
    "    # homogenous agent for parameters sharing, assumes all agents have same obs and action dim\n",
    "    action_dim: int\n",
    "    hidden_dim: int\n",
    "    init_scale: float = 1.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, hidden, obs, dones):\n",
    "        embedding = nn.Dense(\n",
    "            self.hidden_dim,\n",
    "            kernel_init=orthogonal(self.init_scale),\n",
    "            bias_init=constant(0.0),\n",
    "        )(obs)\n",
    "        embedding = nn.relu(embedding)\n",
    "\n",
    "        rnn_in = (embedding, dones)\n",
    "        hidden, embedding = ScannedRNN()(hidden, rnn_in)\n",
    "\n",
    "        q_vals = nn.Dense(\n",
    "            self.action_dim,\n",
    "            kernel_init=orthogonal(self.init_scale),\n",
    "            bias_init=constant(0.0),\n",
    "        )(embedding)\n",
    "\n",
    "        return hidden, q_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0cb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init environment\n",
    "#env = environments.create(env_name, episode_length=episode_length)\n",
    "scenario = map_name_to_scenario(config[\"MAP_NAME\"])\n",
    "env = HeuristicEnemySMAX(scenario=scenario, **config[\"ENV_KWARGS\"])\n",
    "env = SMAXLogWrapper(env)\n",
    "wrapped_env = CTRolloutManager(env, batch_size=config[\"NUM_ENVS\"])\n",
    "\n",
    "rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "wrapped_env = CTRolloutManager(env, batch_size=config[\"NUM_ENVS\"])\n",
    "reset_fn = jax.jit(wrapped_env.batch_reset)\n",
    "#reset_fn = jax.jit(env.reset)\n",
    "\n",
    "config[\"NUM_ACTORS\"] = env.num_agents * config[\"NUM_ENVS\"]\n",
    "config[\"NUM_UPDATES\"] = (\n",
    "    config[\"TOTAL_TIMESTEPS\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS\"]\n",
    ")\n",
    "\n",
    "\n",
    "# Init a random key\n",
    "key = jax.random.key(seed)\n",
    "\n",
    "# Init policy network\n",
    "policy_layer_sizes = policy_hidden_layer_sizes + (env.action_space(env.agents[0]).n,) # Not actually used\n",
    "#policy_network = ActorRNN(env.action_space(env.agents[0]).n, config=config)\n",
    "policy_network = RNNQNetwork(\n",
    "            action_dim=wrapped_env.max_action_space,\n",
    "            hidden_dim=config[\"HIDDEN_SIZE\"],\n",
    "        )\n",
    "\n",
    "# Init population of controllers\n",
    "key, subkey = jax.random.split(key)\n",
    "#keys = jax.random.split(subkey, num=batch_size)\n",
    "# fake_batch = jnp.zeros(shape=(batch_size, env.observation_size)) # Not needed as on-policy is used for no batches?\n",
    "# init_variables = jax.vmap(policy_network.init)(keys, fake_batch)\n",
    "init_x = (\n",
    "    jnp.zeros(\n",
    "        (1, 1, wrapped_env.obs_size)\n",
    "    ),  # (time_step, batch_size, obs_size)\n",
    "    jnp.zeros((1, 1)),  # (time_step, batch size)\n",
    ")\n",
    "init_hstate = ScannedRNN.initialize_carry(\n",
    "    config[\"HIDDEN_SIZE\"], 1\n",
    ")  # (batch_size, hidden_dim)\n",
    "\n",
    "keys = jax.random.split(subkey, batch_size)\n",
    "# Assume init_hstate: [H], init_x: [X]\n",
    "fake_hstate = jnp.stack([init_hstate] * batch_size)  # [N, H]\n",
    "#fake_x = jnp.stack([init_x] * config[\"NUM_ENVS\"])            # [N, X]\n",
    "fake_x = tuple(jnp.stack([x] * batch_size) for x in init_x)\n",
    "\n",
    "#network_params = policy_network.init(subkey, init_hstate, init_x)\n",
    "network_params = jax.vmap(\n",
    "    policy_network.init\n",
    ")(keys, fake_hstate, *fake_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "363f8d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transition(NamedTuple):\n",
    "    global_done: jnp.ndarray\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "    env_state: jnp.ndarray\n",
    "    info: jnp.ndarray\n",
    "    avail_actions: jnp.ndarray\n",
    "\n",
    "\n",
    "class Timestep:\n",
    "    obs: dict\n",
    "    actions: dict\n",
    "    rewards: dict\n",
    "    dones: dict\n",
    "    avail_actions: dict\n",
    "\n",
    "\n",
    "class CustomTrainState(TrainState):\n",
    "    target_network_params: Any\n",
    "    timesteps: int = 0\n",
    "    n_updates: int = 0\n",
    "    grad_steps: int = 0\n",
    "\n",
    "def batchify(x: dict):\n",
    "    return jnp.stack([x[agent] for agent in env.agents], axis=0)\n",
    "\n",
    "# Used for training multiple parent policies in parallel\n",
    "def batchify_multi(x_batch: list[dict]):\n",
    "    # x_batch is a list/array of length batch_size, each an obs dict\n",
    "    return jax.vmap(batchify)(x_batch)  \n",
    "    # → [batch_size, num_agents, num_envs, obs_dim]\n",
    "\n",
    "def unbatchify(x: jnp.ndarray):\n",
    "    return {agent: x[i] for i, agent in enumerate(env.agents)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f024b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to play a step with the policy in the environment\n",
    "def play_step_fn(\n",
    "    runner_state\n",
    "):\n",
    "    \"\"\"\n",
    "    Play an environment step and return the updated state and the transition.\n",
    "    \"\"\"\n",
    "    #hs, last_obs, last_dones, env_state, rng = carry\n",
    "    policy_params, env_state, last_obs, last_dones, hstate, rng = runner_state\n",
    "\n",
    "    rng, rng_a, rng_s = jax.random.split(rng, 3)\n",
    "\n",
    "    # (num_agents, 1 (dummy time), num_envs, obs_size)\n",
    "    _obs = batchify(last_obs)[:, np.newaxis]\n",
    "    _dones = batchify(last_dones)[:, np.newaxis]\n",
    "    #_obs = batchify_multi(last_obs)[:, :, np.newaxis]    # [batch, num_agents, 1, num_envs, obs_dim]\n",
    "    #_dones = batchify_multi(last_dones)[:, :, np.newaxis] # [batch, num_agents, 1, num_envs]\n",
    "\n",
    "    print(\"hstate shape:\", hstate.shape)  # should be [BATCH_SIZE, NUM_ENVS, NUM_AGENTS, HIDDEN_SIZE]\n",
    "    print(\"_obs shape:\", _obs.shape)      # should be [BATCH_SIZE, NUM_ENVS, OBS_DIM]\n",
    "    print(\"_dones shape:\", _dones.shape)  # should be [BATCH_SIZE, NUM_ENVS, OBS_DIM]\n",
    "\n",
    "    new_hstate, q_vals = jax.vmap(\n",
    "        policy_network.apply, in_axes=(None, 0, 0, 0)\n",
    "    )(  # vmap across the agent dim\n",
    "        policy_params,\n",
    "        hstate,\n",
    "        _obs,\n",
    "        _dones,\n",
    "    )\n",
    "    print(\"new_hstate shape:\", new_hstate.shape)  # should be [BATCH_SIZE, NUM_ENVS, NUM_AGENTS, HIDDEN_SIZE]\n",
    "\n",
    "    q_vals = q_vals.squeeze(\n",
    "        axis=1\n",
    "    )  # (num_agents, num_envs, num_actions) remove the time dim\n",
    "\n",
    "    # explore\n",
    "    avail_actions = wrapped_env.get_valid_actions(env_state.env_state)\n",
    "\n",
    "    #eps = eps_scheduler(train_state.n_updates)\n",
    "    #_rngs = jax.random.split(rng_a, env.num_agents)\n",
    "    #actions = jax.vmap(eps_greedy_exploration, in_axes=(0, 0, None, 0))(\n",
    "    #    _rngs, q_vals, eps, batchify(avail_actions)\n",
    "    #)\n",
    "    \n",
    "    unavail_actions = 1 - avail_actions\n",
    "    q_vals = q_vals - (unavail_actions * 1e10)\n",
    "    actions = jnp.argmax(q_vals, axis=-1)\n",
    "    actions = unbatchify(actions)\n",
    "    print(\"actions\", actions)\n",
    "\n",
    "\n",
    "    new_obs, new_env_state, rewards, dones, infos = wrapped_env.batch_step(\n",
    "        rng_s, env_state, actions\n",
    "    )\n",
    "    \n",
    "    #timestep = Timestep(\n",
    "    #    obs=last_obs,\n",
    "    #    actions=actions,\n",
    "    #    rewards=jax.tree.map(lambda x:config.get(\"REW_SCALE\", 1)*x, rewards),\n",
    "    #    dones=last_dones,\n",
    "    #    avail_actions=avail_actions,\n",
    "    #)\n",
    "\n",
    "    transition = Transition(\n",
    "        jnp.tile(dones[\"__all__\"], env.num_agents),\n",
    "        last_dones,\n",
    "        actions.squeeze(),\n",
    "        batchify(rewards, env.agents, config[\"NUM_ACTORS\"]).squeeze(),\n",
    "        _obs,\n",
    "        env_state.env_state.state,\n",
    "        infos,\n",
    "        avail_actions,\n",
    "    )\n",
    "\n",
    "    runner_state = (policy_params, new_env_state, new_obs, dones, new_hstate, rng)\n",
    "    \n",
    "    return runner_state, transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15925bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unroll(\n",
    "    runner_state,\n",
    "    episode_length: int,\n",
    "    play_step_fn: Callable[\n",
    "        [EnvState, Params, RNGKey],\n",
    "        Tuple[\n",
    "            EnvState,\n",
    "            Params,\n",
    "            RNGKey,\n",
    "            Transition,\n",
    "        ],\n",
    "    ],\n",
    ") -> Tuple[EnvState, Transition]:\n",
    "    \"\"\"Generates an episode according to the agent's policy, returns the final state of\n",
    "    the episode and the transitions of the episode.\n",
    "\n",
    "    Args:\n",
    "        init_state: first state of the rollout.\n",
    "        policy_params: params of the individual.\n",
    "        key: random key for stochasiticity handling.\n",
    "        episode_length: length of the rollout.\n",
    "        play_step_fn: function describing how a step need to be taken.\n",
    "\n",
    "    Returns:\n",
    "        A new state, the experienced transition.\n",
    "    \"\"\"\n",
    "\n",
    "    def _scan_play_step_fn(\n",
    "        carry, unused_arg: Any #: Tuple[EnvState, Params, RNGKey]\n",
    "    ) -> Tuple[Tuple[EnvState, Params, RNGKey], Transition]:\n",
    "        runner_state, transitions = play_step_fn(carry)\n",
    "        return runner_state, transitions\n",
    "\n",
    "    runner_state, transitions = jax.lax.scan(\n",
    "        _scan_play_step_fn,\n",
    "        runner_state,\n",
    "        (),\n",
    "        length=episode_length,\n",
    "    )\n",
    "    return runner_state, transitions\n",
    "\n",
    "\n",
    "def get_mask_from_transitions(\n",
    "    data: Transition,\n",
    ") -> jnp.ndarray:\n",
    "    is_done = jnp.clip(jnp.cumsum(data.global_done, axis=1), 0, 1)\n",
    "    mask = jnp.roll(is_done, 1, axis=1)\n",
    "    mask = mask.at[:, 0].set(0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871dec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring_function(\n",
    "    policy_params: Genotype,\n",
    "    key: RNGKey,\n",
    "    episode_length: int,\n",
    "    play_reset_fn: Callable[[RNGKey], EnvState],\n",
    "    play_step_fn: Callable[\n",
    "        [EnvState, Params, RNGKey], Tuple[EnvState, Params, RNGKey, QDTransition]\n",
    "    ],\n",
    "    descriptor_extractor: Callable[[QDTransition, jnp.ndarray], Descriptor],\n",
    ") -> Tuple[Fitness, Descriptor, ExtraScores]:\n",
    "    \"\"\"Evaluates policies contained in policies_params in parallel.\n",
    "    The play_reset_fn function allows for a more general scoring_function that can be\n",
    "    called with different batch-size and not only with a batch-size of the same\n",
    "    dimension as init_states.\n",
    "\n",
    "    To define purely stochastic environments, using the reset function from the\n",
    "    environment, use \"play_reset_fn = env.reset\".\n",
    "\n",
    "    To define purely deterministic environments, as in \"scoring_function\", generate\n",
    "    a single init_state using \"init_state = env.reset(key)\", then use\n",
    "    \"play_reset_fn = lambda key: init_state\".\n",
    "\n",
    "    Args:\n",
    "        policies_params: The parameters of closed-loop controllers/policies to evaluate.\n",
    "        key: A jax random key\n",
    "        episode_length: The maximal rollout length.\n",
    "        play_reset_fn: The function to reset the environment and obtain initial states.\n",
    "        play_step_fn: The function to play a step of the environment.\n",
    "        descriptor_extractor: The function to extract the descriptor.\n",
    "\n",
    "    Returns:\n",
    "        fitness: Array of fitnesses of all evaluated policies\n",
    "        descriptor: Behavioural descriptors of all evaluated policies\n",
    "        extra_scores: Additional information resulting from the evaluation\n",
    "    \"\"\"\n",
    "\n",
    "    # Reset environments\n",
    "    key, subkey = jax.random.split(key)\n",
    "    #keys = jax.random.split(subkey, jax.tree.leaves(policies_params)[0].shape[0])\n",
    "    keys = jax.random.split(subkey, config[\"NUM_ENVS\"])\n",
    "    init_obs, init_env_state = jax.vmap(play_reset_fn, in_axes=(0,))(keys)\n",
    "    \n",
    "    keys = jax.random.split(key, batch_size)\n",
    "\n",
    "    #init_dones = {\n",
    "    #    agent: jnp.zeros((config[\"NUM_ENVS\"]), dtype=bool)\n",
    "    #    for agent in env.agents + [\"__all__\"]\n",
    "    #}\n",
    "    init_hstate = ScannedRNN.initialize_carry(\n",
    "        config[\"HIDDEN_SIZE\"], len(env.agents), config[\"NUM_ENVS\"]\n",
    "    )\n",
    "    \n",
    "    batched_env_state = jax.tree.map(lambda x: jnp.stack([x] * batch_size), init_env_state)\n",
    "    batched_obs = jax.tree.map(lambda x: jnp.stack([x] * batch_size), init_obs)\n",
    "    batched_dones = {\n",
    "        agent: jnp.stack([jnp.zeros(config[\"NUM_ENVS\"], dtype=bool)] * batch_size)\n",
    "        for agent in env.agents + [\"__all__\"]\n",
    "    }\n",
    "    batched_hstate = jnp.stack([init_hstate] * batch_size)\n",
    "\n",
    "    print(\"batched_hstate shape:\", batched_hstate.shape)  \n",
    "\n",
    "    init_runner_state = (policy_params, batched_env_state, batched_obs, batched_dones, batched_hstate, keys)\n",
    "\n",
    "    # Step environments\n",
    "    unroll_fn = functools.partial(\n",
    "        generate_unroll,\n",
    "        episode_length=episode_length,\n",
    "        play_step_fn=play_step_fn,\n",
    "    )\n",
    "    #keys = jax.random.split(key, jax.tree.leaves(policies_params)[0].shape[0])\n",
    "    _, data = jax.vmap(unroll_fn)(init_runner_state) # data = Transistions data struc\n",
    "\n",
    "    # Create a mask to extract data properly\n",
    "    mask = get_mask_from_transitions(data)\n",
    "\n",
    "    # Evaluate\n",
    "    print(\"data.reward shape:\", data.reward.shape)\n",
    "    #fitnesses = jnp.sum(data.reward * (1.0 - mask), axis=1)\n",
    "    fitnesses = jnp.sum(data.reward * (1.0 - mask), axis=(1, 2))  # shape (32,)\n",
    "    fitnesses = fitnesses.reshape(-1, 1)  # shape (32, 1)\n",
    "    descriptors = descriptor_extractor(data, mask)\n",
    "\n",
    "    print(\"fitness shape:\", fitnesses.shape)\n",
    "    print(\"descriptors shape:\", descriptors.shape)\n",
    "\n",
    "    return fitnesses, descriptors, {\"transitions\": data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b2edde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptors(data):\n",
    "    \"\"\"\n",
    "    Compute per-trajectory descriptors:\n",
    "    - % of units that moved last step (per batch item)\n",
    "    - % of units that attacked last step (per batch item)\n",
    "    - Avg ally-ally distance at final step (per batch item)\n",
    "    \n",
    "    Output: shape (batch_size, 3)\n",
    "    \"\"\"\n",
    "    movement_actions = data.env_state.prev_movement_actions          # (B, T, E, A, 2)\n",
    "    attack_actions = data.env_state.prev_attack_actions              # (B, T, E, A)\n",
    "    alive = data.env_state.unit_alive.astype(jnp.float32)            # (B, T, E, A)\n",
    "    positions = data.env_state.unit_positions                        # (B, T, E, A, 2)\n",
    "    teams = data.env_state.unit_teams                                # (B, T, E, A)\n",
    "\n",
    "    moved = jnp.any(movement_actions != 0.0, axis=-1)                # (B, T, E, A)\n",
    "    attacked = attack_actions > 0                                    # (B, T, E, A)\n",
    "    is_ally = (teams == 0).astype(jnp.float32)                        # (B, T, E, A)\n",
    "    ally_alive = alive * is_ally                                      # (B, T, E, A)\n",
    "\n",
    "    # Per batch movement/attack % (sum over all but batch dim)\n",
    "    def pct_nonzero(mask, alive_mask):\n",
    "        numerator = jnp.sum(mask * alive_mask, axis=(1, 2, 3))        # (B,)\n",
    "        denominator = jnp.sum(alive_mask, axis=(1, 2, 3)) + 1e-8      # (B,)\n",
    "        return numerator / denominator                                # (B,)\n",
    "\n",
    "    movement_pct = pct_nonzero(moved, ally_alive)                    # (B,)\n",
    "    attack_pct = pct_nonzero(attacked, ally_alive)                   # (B,)\n",
    "\n",
    "    # Final step only for positions and masks\n",
    "    final_positions = positions[:, -1, :, :, :]                      # (B, E, A, 2)\n",
    "    final_alive = alive[:, -1, :, :]                                 # (B, E, A)\n",
    "    final_teams = teams[:, -1, :, :]                                 # (B, E, A)\n",
    "    ally_mask = (final_teams == 0) * final_alive                     # (B, E, A)\n",
    "\n",
    "    def avg_pairwise_distance(pos, mask):\n",
    "        \"\"\"\n",
    "        pos: (A, 2)\n",
    "        mask: (A,)\n",
    "        returns: scalar distance\n",
    "        \"\"\"\n",
    "        diffs = pos[:, None, :] - pos[None, :, :]                    # (A, A, 2)\n",
    "        dists = jnp.linalg.norm(diffs, axis=-1)                      # (A, A)\n",
    "        pair_mask = mask[:, None] * mask[None, :]                    # (A, A)\n",
    "        not_self = 1 - jnp.eye(mask.shape[0])\n",
    "        valid_pairs = pair_mask * not_self\n",
    "        total_dist = jnp.sum(dists * valid_pairs)\n",
    "        num_pairs = jnp.sum(valid_pairs)\n",
    "        return jnp.where(num_pairs > 0, total_dist / num_pairs, 0.0)\n",
    "\n",
    "    # vmap over batch and envs: returns shape (B, E)\n",
    "    avg_dists = jax.vmap(\n",
    "        jax.vmap(avg_pairwise_distance, in_axes=(0, 0)),\n",
    "        in_axes=(0, 0)\n",
    "    )(final_positions, ally_mask)\n",
    "\n",
    "    avg_dist = jnp.mean(avg_dists, axis=1)  # mean over envs → shape (B,)\n",
    "\n",
    "    # Normalize and clip\n",
    "    max_distance = 32\n",
    "    quarter_max = 0.25 * max_distance \n",
    "    norm_dist = avg_dist / quarter_max\n",
    "    norm_dist = jnp.clip(norm_dist, 0.0, 1.0)\n",
    "\n",
    "    # Stack all descriptors → shape (B, 3)\n",
    "    #return jnp.stack([movement_pct, attack_pct, norm_dist], axis=-1)\n",
    "    return jnp.stack([movement_pct, norm_dist], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "def descriptor_extraction_fn(data: Transition, mask: jnp.ndarray) -> Descriptor:\n",
    "    \"\"\"Compute final xy position.\n",
    "\n",
    "    This function suppose that state descriptor is the xy position, as it\n",
    "    just select the final one of the state descriptors given.\n",
    "    \"\"\"\n",
    "    # reshape mask for descriptor extraction\n",
    "    mask = jnp.expand_dims(mask, axis=-1)\n",
    "\n",
    "    # Get descriptor\n",
    "    #last_index = jnp.int32(jnp.sum(1.0 - mask, axis=1)) - 1\n",
    "    #descriptors = jax.vmap(lambda x, y: x[y])(data.state_desc, last_index)\n",
    "    descriptors = compute_descriptors(data)\n",
    "\n",
    "    # remove the dim coming from the trajectory\n",
    "    return descriptors#.squeeze(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf096879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the scoring function\n",
    "# descriptor_extraction_fn = environments.descriptor_extractor[env_name] # Need to write my own function to extract descriptors\n",
    "scoring_fn = functools.partial(\n",
    "    scoring_function,\n",
    "    episode_length=episode_length,\n",
    "    play_reset_fn=reset_fn,\n",
    "    play_step_fn=play_step_fn,\n",
    "    descriptor_extractor=descriptor_extraction_fn,\n",
    ")\n",
    "\n",
    "# Get minimum reward value to make sure qd_score are positive\n",
    "#reward_offset = environments.reward_offset[env_name]\n",
    "\n",
    "# Define a metrics function\n",
    "metrics_function = functools.partial(\n",
    "    default_qd_metrics,\n",
    "    qd_offset=1#reward_offset * episode_length, # Used to ensure QD score is positive could set to 1 if not needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94bdbaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define emitter\n",
    "variation_fn = functools.partial(\n",
    "    isoline_variation, iso_sigma=iso_sigma, line_sigma=line_sigma\n",
    ")\n",
    "\n",
    "mixing_emitter = MixingEmitter(\n",
    "    mutation_fn=None,\n",
    "    variation_fn=variation_fn,\n",
    "    variation_percentage=1.0,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7dfea00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hstate shape: (3, 128, 512)\n",
      "_obs shape: (3, 1, 128, 128, 104)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "scan body function carry input and carry output must have equal types, but they differ:\n\nThe input carry component c[1] has type float32[128,512] but the corresponding output carry component has type float32[128,128,512], so the shapes do not match.\n\nRevise the function so that all output types match the corresponding input types.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Compute initial repertoire and emitter state\u001b[39;00m\n\u001b[32m     20\u001b[39m key, subkey = jax.random.split(key)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m repertoire, emitter_state, init_metrics = \u001b[43mmap_elites\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcentroids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/eww24/Masters_project/JaxMARL/lib/python3.12/site-packages/qdax/core/map_elites.py:82\u001b[39m, in \u001b[36mMAPElites.init\u001b[39m\u001b[34m(self, genotypes, centroids, key)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# score initial genotypes\u001b[39;00m\n\u001b[32m     81\u001b[39m key, subkey = jax.random.split(key)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m (fitnesses, descriptors, extra_scores) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_scoring_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgenotypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubkey\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m repertoire, emitter_state, metrics = \u001b[38;5;28mself\u001b[39m.init_ask_tell(\n\u001b[32m     87\u001b[39m     genotypes=genotypes,\n\u001b[32m     88\u001b[39m     fitnesses=fitnesses,\n\u001b[32m   (...)\u001b[39m\u001b[32m     92\u001b[39m     extra_scores=extra_scores,\n\u001b[32m     93\u001b[39m )\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m repertoire, emitter_state, metrics\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 70\u001b[39m, in \u001b[36mscoring_function\u001b[39m\u001b[34m(policy_params, key, episode_length, play_reset_fn, play_step_fn, descriptor_extractor)\u001b[39m\n\u001b[32m     64\u001b[39m unroll_fn = functools.partial(\n\u001b[32m     65\u001b[39m     generate_unroll,\n\u001b[32m     66\u001b[39m     episode_length=episode_length,\n\u001b[32m     67\u001b[39m     play_step_fn=play_step_fn,\n\u001b[32m     68\u001b[39m )\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m#keys = jax.random.split(key, jax.tree.leaves(policies_params)[0].shape[0])\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m _, data = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43munroll_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_runner_state\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# data = Transistions data struc\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Create a mask to extract data properly\u001b[39;00m\n\u001b[32m     73\u001b[39m mask = get_mask_from_transitions(data)\n",
      "    \u001b[31m[... skipping hidden 7 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mgenerate_unroll\u001b[39m\u001b[34m(runner_state, episode_length, play_step_fn)\u001b[39m\n\u001b[32m     31\u001b[39m     runner_state, transitions = play_step_fn(carry)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner_state, transitions\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m runner_state, transitions = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_scan_play_step_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunner_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepisode_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m runner_state, transitions\n",
      "    \u001b[31m[... skipping hidden 10 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mgenerate_unroll.<locals>._scan_play_step_fn\u001b[39m\u001b[34m(carry, unused_arg)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_scan_play_step_fn\u001b[39m(\n\u001b[32m     29\u001b[39m     carry, unused_arg: Any \u001b[38;5;66;03m#: Tuple[EnvState, Params, RNGKey]\u001b[39;00m\n\u001b[32m     30\u001b[39m ) -> Tuple[Tuple[EnvState, Params, RNGKey], Transition]:\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     runner_state, transitions = \u001b[43mplay_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner_state, transitions\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mplay_step_fn\u001b[39m\u001b[34m(runner_state)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mhstate shape:\u001b[39m\u001b[33m\"\u001b[39m, hstate.shape)  \u001b[38;5;66;03m# should be [BATCH_SIZE, NUM_ENVS, NUM_AGENTS, HIDDEN_SIZE]\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m_obs shape:\u001b[39m\u001b[33m\"\u001b[39m, _obs.shape)      \u001b[38;5;66;03m# should be [BATCH_SIZE, NUM_ENVS, OBS_DIM]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m new_hstate, q_vals = \u001b[43mjax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicy_network\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# vmap across the agent dim\u001b[39;49;00m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicy_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_obs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_dones\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m q_vals = q_vals.squeeze(\n\u001b[32m     31\u001b[39m     axis=\u001b[32m1\u001b[39m\n\u001b[32m     32\u001b[39m )  \u001b[38;5;66;03m# (num_agents, num_envs, num_actions) remove the time dim\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# explore\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 13 frame]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mRNNQNetwork.__call__\u001b[39m\u001b[34m(self, hidden, obs, dones)\u001b[39m\n\u001b[32m     45\u001b[39m embedding = nn.relu(embedding)\n\u001b[32m     47\u001b[39m rnn_in = (embedding, dones)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m hidden, embedding = \u001b[43mScannedRNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn_in\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m q_vals = nn.Dense(\n\u001b[32m     51\u001b[39m     \u001b[38;5;28mself\u001b[39m.action_dim,\n\u001b[32m     52\u001b[39m     kernel_init=orthogonal(\u001b[38;5;28mself\u001b[39m.init_scale),\n\u001b[32m     53\u001b[39m     bias_init=constant(\u001b[32m0.0\u001b[39m),\n\u001b[32m     54\u001b[39m )(embedding)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m hidden, q_vals\n",
      "    \u001b[31m[... skipping hidden 3 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/eww24/Masters_project/JaxMARL/lib/python3.12/site-packages/flax/core/axes_scan.py:179\u001b[39m, in \u001b[36mscan.<locals>.scan_fn\u001b[39m\u001b[34m(broadcast_in, init, *args)\u001b[39m\n\u001b[32m    174\u001b[39m broadcast_in, constants_out = jax.tree_util.tree_unflatten(\n\u001b[32m    175\u001b[39m     out_tree(), out_flat\n\u001b[32m    176\u001b[39m )\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jax.version.__version_info__ > (\u001b[32m0\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m25\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m   c, ys = \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscan\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m      \u001b[49m\u001b[43mbody_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munroll\u001b[49m\u001b[43m=\u001b[49m\u001b[43munroll\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_split_transpose\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_split_transpose\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    184\u001b[39m   c, ys = lax.scan(\n\u001b[32m    185\u001b[39m       body_fn, init, xs, length=length, reverse=reverse, unroll=unroll\n\u001b[32m    186\u001b[39m   )\n",
      "    \u001b[31m[... skipping hidden 2 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/vol/bitbucket/eww24/Masters_project/JaxMARL/lib/python3.12/site-packages/jax/_src/lax/control_flow/loops.py:466\u001b[39m, in \u001b[36m_check_carry_type\u001b[39m\u001b[34m(name, body_fun, in_carry, out_carry_tree, out_avals)\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pvary_msg:\n\u001b[32m    463\u001b[39m   pvary_msg += (\u001b[33m\"\u001b[39m\u001b[33mSee https://docs.jax.dev/en/latest/notebooks/shard_map.html#scan-vma \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    464\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfor more information.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    467\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m function carry input and carry output must have equal types, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    468\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbut they differ:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    469\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdifferences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    470\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpvary_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    471\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRevise the function so that all output types match the corresponding \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    472\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minput types.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: scan body function carry input and carry output must have equal types, but they differ:\n\nThe input carry component c[1] has type float32[128,512] but the corresponding output carry component has type float32[128,128,512], so the shapes do not match.\n\nRevise the function so that all output types match the corresponding input types."
     ]
    }
   ],
   "source": [
    "# Instantiate MAP-Elites\n",
    "map_elites = MAPElites(\n",
    "    scoring_function=scoring_fn,\n",
    "    emitter=mixing_emitter,\n",
    "    metrics_function=metrics_function,\n",
    ")\n",
    "\n",
    "# Compute the centroids\n",
    "key, subkey = jax.random.split(key)\n",
    "centroids = compute_cvt_centroids(\n",
    "    num_descriptors=number_of_descriptors,#env.descriptor_length, # Num of dimensions in the descriptor\n",
    "    num_init_cvt_samples=num_init_cvt_samples,\n",
    "    num_centroids=num_centroids,\n",
    "    minval=min_descriptor,\n",
    "    maxval=max_descriptor,\n",
    "    key=subkey,\n",
    ")\n",
    "\n",
    "# Compute initial repertoire and emitter state\n",
    "key, subkey = jax.random.split(key)\n",
    "repertoire, emitter_state, init_metrics = map_elites.init(network_params, centroids, subkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d251ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_period = 10\n",
    "num_loops = num_iterations // log_period\n",
    "\n",
    "# Initialize metrics\n",
    "metrics = {key: jnp.array([]) for key in [\"iteration\", \"qd_score\", \"coverage\", \"max_fitness\", \"time\"]}\n",
    "\n",
    "# Set up init metrics\n",
    "init_metrics = jax.tree.map(lambda x: jnp.array([x]) if x.shape == () else x, init_metrics)\n",
    "init_metrics[\"iteration\"] = jnp.array([0], dtype=jnp.int32)\n",
    "init_metrics[\"time\"] = jnp.array([0.0])  # No time recorded for initialization\n",
    "\n",
    "# Convert init_metrics to match the metrics dictionary structure\n",
    "metrics = jax.tree.map(lambda metric, init_metric: jnp.concatenate([metric, init_metric], axis=0), metrics, init_metrics)\n",
    "\n",
    "# Initialize CSV logger\n",
    "csv_logger = CSVLogger(\n",
    "    \"mapelites-logs.csv\",\n",
    "    header=list(metrics.keys())\n",
    ")\n",
    "\n",
    "# Log initial metrics\n",
    "csv_logger.log(jax.tree.map(lambda x: x[-1], metrics))\n",
    "\n",
    "# Main loop\n",
    "map_elites_scan_update = map_elites.scan_update\n",
    "for i in range(num_loops):\n",
    "    start_time = time.time()\n",
    "    (\n",
    "        repertoire,\n",
    "        emitter_state,\n",
    "        key,\n",
    "    ), current_metrics = jax.lax.scan(\n",
    "        map_elites_scan_update,\n",
    "        (repertoire, emitter_state, key),\n",
    "        (),\n",
    "        length=log_period,\n",
    "    )\n",
    "    timelapse = time.time() - start_time\n",
    "\n",
    "    # Metrics\n",
    "    current_metrics[\"iteration\"] = jnp.arange(1+log_period*i, 1+log_period*(i+1), dtype=jnp.int32)\n",
    "    current_metrics[\"time\"] = jnp.repeat(timelapse, log_period)\n",
    "    metrics = jax.tree.map(lambda metric, current_metric: jnp.concatenate([metric, current_metric], axis=0), metrics, current_metrics)\n",
    "\n",
    "    # Log\n",
    "    csv_logger.log(jax.tree.map(lambda x: x[-1], metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217deb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the x-axis array\n",
    "#env_steps = metrics[\"iteration\"]\n",
    "env_steps = metrics[\"iteration\"] * batch_size * episode_length * config[\"NUM_ENVS\"]\n",
    "#print(jnp.max(metrics[\"iteration\"]))\n",
    "\n",
    "%matplotlib inline\n",
    "# Create the plots and the grid\n",
    "fig, axes = plot_map_elites_results(env_steps=env_steps, metrics=metrics, repertoire=repertoire, min_descriptor=min_descriptor, max_descriptor=max_descriptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc2cb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = jnp.argmax(repertoire.fitnesses)\n",
    "best_fitness = jnp.max(repertoire.fitnesses)\n",
    "best_descriptor = repertoire.descriptors[best_idx]\n",
    "print(\n",
    "    f\"Best fitness in the repertoire: {best_fitness:.2f}\\n\",\n",
    "    f\"Descriptor of the best individual in the repertoire: {best_descriptor}\\n\",\n",
    "    f\"Index in the repertoire of this individual: {best_idx}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317021d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the parameters of the best individual\n",
    "my_params = jax.tree.map(\n",
    "    lambda x: x[best_idx],\n",
    "    repertoire.genotypes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d65a609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_obs_with_id(obs_dict, env):\n",
    "    \"\"\"Simulate CTRolloutManager's preprocessing by adding one-hot agent IDs.\"\"\"\n",
    "    new_obs_dict = {}\n",
    "    num_agents = len(env.agents)\n",
    "    for i, agent in enumerate(env.agents):\n",
    "        obs = obs_dict[agent].flatten()\n",
    "        one_hot = jax.nn.one_hot(i, num_classes=num_agents)\n",
    "        new_obs_dict[agent] = jnp.concatenate([obs, one_hot])\n",
    "    return new_obs_dict\n",
    "\n",
    "\n",
    "def visualize_recurrent_policy(trained_params, env, config):\n",
    "    rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "    rng, reset_rng = jax.random.split(rng)\n",
    "    #wrapped_env = CTRolloutManager(env, batch_size=1)\n",
    "\n",
    "    # Create policy network\n",
    "    #network = RNNQNetwork(\n",
    "    #    action_dim=wrapped_env.max_action_space,\n",
    "    #    hidden_dim=config[\"HIDDEN_SIZE\"],\n",
    "    #)\n",
    "    network = RNNQNetwork(\n",
    "        action_dim=env.action_space(env.agents[0]).n,\n",
    "        hidden_dim=config[\"HIDDEN_SIZE\"],\n",
    "    )\n",
    "    \n",
    "    # Reset environment\n",
    "    #obs, env_state = wrapped_env.batch_reset(reset_rng)\n",
    "    obs, env_state = env.reset(reset_rng)\n",
    "    #dones = {\n",
    "    #    agent: jnp.zeros((1), dtype=bool)\n",
    "    #    for agent in env.agents + [\"__all__\"]\n",
    "    #}\n",
    "    dones = {agent: jnp.array(False) for agent in env.agents}\n",
    "    hstate = ScannedRNN.initialize_carry(\n",
    "        config[\"HIDDEN_SIZE\"], len(env.agents), 1\n",
    "    )\n",
    "    \n",
    "    # Collect all environment states\n",
    "    returns = {agent: 0.0 for agent in env.agents}\n",
    "    state_seq = []\n",
    "    max_steps = config[\"NUM_STEPS\"]\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        # Compute Q-values\n",
    "        # Prepare inputs for network\n",
    "        obs = preprocess_obs_with_id(obs, env)\n",
    "        _obs = batchify(obs)         # (num_agents, obs_dim)\n",
    "        _obs = _obs[:, None, :]                      # (num_agents, 1, obs_dim)\n",
    "\n",
    "        #_dones = batchify(dones)    # (num_agents,)\n",
    "        #_dones = _dones[:, None]                     # (num_agents, 1)\n",
    "        _dones = jnp.stack([jnp.array([dones[agent]]) for agent in env.agents])  # shape (num_agents, 1)\n",
    "        _dones = jnp.expand_dims(_dones, axis=-1)  # from (3, 1) to (3, 1, 1)\n",
    "\n",
    "        #print(\"_obs.shape:\", _obs.shape)\n",
    "        #print(\"_dones.shape:\", _dones.shape)\n",
    "        #print(\"hstate.shape:\", hstate.shape)\n",
    "\n",
    "        def apply_fn(h, o, d):\n",
    "            return network.apply(trained_params, h, o, d)\n",
    "\n",
    "        hstate, q_vals = jax.vmap(apply_fn, in_axes=(0, 0, 0))(\n",
    "            hstate,\n",
    "            _obs,\n",
    "            _dones,\n",
    "        )\n",
    "        #hstate = hstate[:, None, :]  # Already in (num_agents, hidden_dim)\n",
    "        q_vals = q_vals.squeeze(axis=1)  # (num_agents, num_envs, num_actions) remove the time dim\n",
    "        #print(\"q_vals.shape\", q_vals.shape)\n",
    "        \n",
    "        actions = {}\n",
    "        #avail_actions = wrapped_env.get_valid_actions(env_state.env_state)\n",
    "        avail_actions = env.get_avail_actions(env_state.env_state)\n",
    "\n",
    "        for i, agent in enumerate(env.agents):\n",
    "            avail_agent = avail_actions[agent][None, None, :]  # shape (1, 1, n_actions)\n",
    "            #print(\"avail_agent.shape\", avail_agent.shape)\n",
    "            \n",
    "            unavail_actions = 1 - avail_agent  # shape (1, 1, n_actions)\n",
    "            \n",
    "            # Select Q-values for this agent only\n",
    "            q_agent = q_vals[i][None, None, :]  # shape (1, 1, n_actions)\n",
    "            q_masked = q_agent - (unavail_actions * 1e10)\n",
    "\n",
    "            action = jnp.argmax(q_masked, axis=-1)  # shape (1, 1)\n",
    "            action = action.squeeze()               # scalar\n",
    "            #print(\"action.shape\", action.shape)\n",
    "\n",
    "            # Wrap in array with batch dim\n",
    "            actions[agent] = int(action)    # shape (1,)\n",
    "        \n",
    "        rng, rng_s = jax.random.split(rng)\n",
    "        state_seq.append((rng_s, env_state.env_state, actions))\n",
    "\n",
    "        # Step environment\n",
    "\n",
    "        # Batch the actions dict\n",
    "        # Original actions: {'ally_0': 4, 'ally_1': 4, 'ally_2': 4}\n",
    "        #actions = {k: jnp.array([v]) for k, v in actions.items()}\n",
    "\n",
    "        #obs, env_state, rewards, dones, infos = wrapped_env.batch_step(\n",
    "        #    rng_s, env_state, actions\n",
    "        #)\n",
    "        obs, env_state, rewards, dones, infos = env.step(rng_s, env_state, actions)\n",
    "        returns = {a: returns[a] + rewards[a] for a in env.agents}\n",
    "        \n",
    "        if dones[\"__all__\"]:\n",
    "            break\n",
    "\n",
    "    # Visualization\n",
    "    print(\"Returns:\", returns)\n",
    "\n",
    "    viz = SMAXVisualizer(env, state_seq)\n",
    "    viz.animate(view=False, save_fname=\"trained_iql_rnn.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9653d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visualize_recurrent_policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m env = HeuristicEnemySMAX(scenario=scenario, **config[\u001b[33m\"\u001b[39m\u001b[33mENV_KWARGS\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m      3\u001b[39m env = SMAXLogWrapper(env)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mvisualize_recurrent_policy\u001b[49m(my_params, config)\n",
      "\u001b[31mNameError\u001b[39m: name 'visualize_recurrent_policy' is not defined"
     ]
    }
   ],
   "source": [
    "scenario = map_name_to_scenario(config[\"MAP_NAME\"])\n",
    "env = HeuristicEnemySMAX(scenario=scenario, **config[\"ENV_KWARGS\"])\n",
    "env = SMAXLogWrapper(env)\n",
    "\n",
    "visualize_recurrent_policy(my_params, config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JaxMARL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
